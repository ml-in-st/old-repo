1
00:00:00,060 --> 00:00:04,380
in the last video we distract what is

2
00:00:02,250 --> 00:00:06,150
the deep l-larry neural network and also

3
00:00:04,380 --> 00:00:08,550
talked about the notation we use to

4
00:00:06,150 --> 00:00:10,650
describe such networks in this video you

5
00:00:08,550 --> 00:00:13,769
see how you can perform for propagation

6
00:00:10,650 --> 00:00:16,440
in a deep network as usual let's first

7
00:00:13,769 --> 00:00:18,660
go over what forward propagation will

8
00:00:16,440 --> 00:00:21,330
look like for a single training example

9
00:00:18,660 --> 00:00:22,920
X and then later on we'll talk about the

10
00:00:21,330 --> 00:00:24,810
vectorized version where you want to

11
00:00:22,920 --> 00:00:26,849
carry out forward propagation on the

12
00:00:24,810 --> 00:00:29,660
entire training set at the same time but

13
00:00:26,849 --> 00:00:32,579
some given a single training example X

14
00:00:29,660 --> 00:00:34,800
here's how you compute the activations

15
00:00:32,579 --> 00:00:42,329
of the first layer so for this first

16
00:00:34,800 --> 00:00:48,239
layer you compute z1 equals W 1 times X

17
00:00:42,329 --> 00:00:51,120
plus b1 so W 1 and B 1 of parameters

18
00:00:48,239 --> 00:00:53,879
that affect the activations in layer 1

19
00:00:51,120 --> 00:00:56,899
right where this is there one of the

20
00:00:53,879 --> 00:00:59,280
neural network and then you compute the

21
00:00:56,899 --> 00:01:04,979
activations for that layer to be equal

22
00:00:59,280 --> 00:01:06,810
to G of Z 1 and the destination function

23
00:01:04,979 --> 00:01:09,090
G depends on what layer you're at and

24
00:01:06,810 --> 00:01:11,010
maybe index AB has the activation

25
00:01:09,090 --> 00:01:12,689
function from there 1 so if you do that

26
00:01:11,010 --> 00:01:13,320
you've now computed the activations from

27
00:01:12,689 --> 00:01:18,360
layer 1

28
00:01:13,320 --> 00:01:24,470
how about layer to say that layer well

29
00:01:18,360 --> 00:01:32,189
you would then compute Z 2 equals W 2 A

30
00:01:24,470 --> 00:01:34,950
1 plus B 2 and then so the activation of

31
00:01:32,189 --> 00:01:39,180
layer 2 is the way matrix times the

32
00:01:34,950 --> 00:01:44,270
output of layer 1 so is that value plus

33
00:01:39,180 --> 00:01:49,579
the bias vector for layer 2 and then a2

34
00:01:44,270 --> 00:01:55,770
equals the activation function apply to

35
00:01:49,579 --> 00:01:57,990
z2 ok so that's it for layer 2 and so on

36
00:01:55,770 --> 00:02:00,299
and so forth until you get to the output

37
00:01:57,990 --> 00:02:06,240
layer that's layer 4 where you would

38
00:02:00,299 --> 00:02:09,959
have that Z 4 is equal to the parameters

39
00:02:06,240 --> 00:02:11,780
for that layer times the activations

40
00:02:09,959 --> 00:02:14,569
from the previous layer

41
00:02:11,780 --> 00:02:23,930
Plus that by this vector and then

42
00:02:14,569 --> 00:02:26,720
similarly a four equals G of v4 and so

43
00:02:23,930 --> 00:02:29,900
that's how you compute your estimated

44
00:02:26,720 --> 00:02:35,390
output Y hat so just one thing to notice

45
00:02:29,900 --> 00:02:38,270
X here is also equal to a zero because

46
00:02:35,390 --> 00:02:41,209
the input feature vector X is also the

47
00:02:38,270 --> 00:02:44,000
activations of layer 0 so we scratch out

48
00:02:41,209 --> 00:02:47,000
X and with a cross for X and put a 0

49
00:02:44,000 --> 00:02:48,709
here then you know all of these

50
00:02:47,000 --> 00:02:53,980
equations they see look the same right

51
00:02:48,709 --> 00:03:02,750
the general rule is that VL is equal to

52
00:02:53,980 --> 00:03:05,750
WL times a of L minus 1 plus B L 1 there

53
00:03:02,750 --> 00:03:10,630
and then the activations so that layer

54
00:03:05,750 --> 00:03:16,850
is the activation function applied to

55
00:03:10,630 --> 00:03:20,120
the values Z so that's the general for

56
00:03:16,850 --> 00:03:23,540
propagation equation so we've done all

57
00:03:20,120 --> 00:03:26,299
this for a single training example how

58
00:03:23,540 --> 00:03:29,660
about for doing it in a vectorized way

59
00:03:26,299 --> 00:03:32,720
for the whole training set at the same

60
00:03:29,660 --> 00:03:35,030
time the equations look quite similar as

61
00:03:32,720 --> 00:03:40,060
before for the first layer you would

62
00:03:35,030 --> 00:03:48,410
have Capital Z 1 equals W 1 times

63
00:03:40,060 --> 00:03:54,650
capital X plus B 1 and then a 1 equals G

64
00:03:48,410 --> 00:03:57,920
of Z 1 right and bearing in mind that X

65
00:03:54,650 --> 00:03:59,959
is equal to a 0 these are just you know

66
00:03:57,920 --> 00:04:01,850
the training examples stacked in

67
00:03:59,959 --> 00:04:05,450
different columns you could take this

68
00:04:01,850 --> 00:04:08,269
let me scratch out X so you can put a 0

69
00:04:05,450 --> 00:04:08,720
there and then so the next layer looks

70
00:04:08,269 --> 00:04:16,720
similar

71
00:04:08,720 --> 00:04:21,980
Z 2 equals W 2 A 1 plus B 2 and a 2

72
00:04:16,720 --> 00:04:24,530
equals G of Z 2

73
00:04:21,980 --> 00:04:28,370
we're just taking these vectors e or a

74
00:04:24,530 --> 00:04:29,810
and so on and stacking them up so this

75
00:04:28,370 --> 00:04:34,310
is V vector for the first training

76
00:04:29,810 --> 00:04:37,310
example V vector for the second training

77
00:04:34,310 --> 00:04:39,830
example and so on down to the M train

78
00:04:37,310 --> 00:04:43,700
example and stacking these in columns

79
00:04:39,830 --> 00:04:47,390
and calling this Capital Z all right and

80
00:04:43,700 --> 00:04:50,000
similarly for capital A just as capital

81
00:04:47,390 --> 00:04:52,040
X all the training examples are column

82
00:04:50,000 --> 00:04:53,720
vectors smacks left to right and then

83
00:04:52,040 --> 00:04:59,450
again end of this process you end up

84
00:04:53,720 --> 00:05:03,200
with Y hat which is equal to G of v4 so

85
00:04:59,450 --> 00:05:04,670
this is also equal to a 4 and that's the

86
00:05:03,200 --> 00:05:08,000
predictions on all the view training

87
00:05:04,670 --> 00:05:09,980
examples is stacked horizontally so just

88
00:05:08,000 --> 00:05:12,590
to summarize our notation I'm going to

89
00:05:09,980 --> 00:05:17,720
modify this up here our notation allows

90
00:05:12,590 --> 00:05:19,820
us to replace lowercase Z and a with the

91
00:05:17,720 --> 00:05:22,070
uppercase counterparts is that already

92
00:05:19,820 --> 00:05:23,810
looks like a capital D and that gives

93
00:05:22,070 --> 00:05:25,790
you the vectorized version the fourth

94
00:05:23,810 --> 00:05:29,060
obligation that you carry out on the

95
00:05:25,790 --> 00:05:32,990
entire training set at a time where a 0

96
00:05:29,060 --> 00:05:35,240
is X now if you look at this

97
00:05:32,990 --> 00:05:37,670
implementation of vectorization it looks

98
00:05:35,240 --> 00:05:40,370
like that there is going to be a for

99
00:05:37,670 --> 00:05:44,360
loop here right so it's left for l

100
00:05:40,370 --> 00:05:47,000
equals 1 to 4 for l equals 1 through

101
00:05:44,360 --> 00:05:48,950
capital L then you have to compute the

102
00:05:47,000 --> 00:05:51,860
activations for layer 1 then for layer 2

103
00:05:48,950 --> 00:05:54,370
then to layer 3 and then 4 therefore so

104
00:05:51,860 --> 00:05:56,660
seems that there is a for loop here and

105
00:05:54,370 --> 00:05:58,550
I know that when implementing your

106
00:05:56,660 --> 00:06:00,770
networks we usually want to get rid of

107
00:05:58,550 --> 00:06:03,290
explicit for loops but this is one place

108
00:06:00,770 --> 00:06:05,060
where I don't think there's any way to

109
00:06:03,290 --> 00:06:06,590
implement this over other than explicit

110
00:06:05,060 --> 00:06:09,080
for loop so we're in implementing for

111
00:06:06,590 --> 00:06:10,700
propagation it is perfectly OK to have a

112
00:06:09,080 --> 00:06:12,740
for loop they compute the activations

113
00:06:10,700 --> 00:06:15,050
for layer 1 then there are 2 then they

114
00:06:12,740 --> 00:06:17,210
are threes and therefore no one knows

115
00:06:15,050 --> 00:06:19,970
and I don't think there is this any way

116
00:06:17,210 --> 00:06:23,060
to do this without a for loops that goes

117
00:06:19,970 --> 00:06:24,620
from 1 to capital L from 1 through the

118
00:06:23,060 --> 00:06:27,830
total number of layers and in your

119
00:06:24,620 --> 00:06:30,980
network so this place is perfectly okay

120
00:06:27,830 --> 00:06:32,690
to have an explicit form so

121
00:06:30,980 --> 00:06:35,300
that's it for the notation for deep

122
00:06:32,690 --> 00:06:37,760
neural networks as well as how to do for

123
00:06:35,300 --> 00:06:39,680
propagation in these networks if the

124
00:06:37,760 --> 00:06:41,900
pieces we've seen so far looks a little

125
00:06:39,680 --> 00:06:44,000
bit familiar to you that's because what

126
00:06:41,900 --> 00:06:45,830
we've seen is taking a piece very

127
00:06:44,000 --> 00:06:47,750
similar to what you've seen in the

128
00:06:45,830 --> 00:06:50,750
neural network with a single hidden

129
00:06:47,750 --> 00:06:53,420
layer and just repeating that more times

130
00:06:50,750 --> 00:06:55,420
now turns out that we implemented deep

131
00:06:53,420 --> 00:06:57,860
neural network one of the ways to

132
00:06:55,420 --> 00:06:59,450
increase your odds of having above free

133
00:06:57,860 --> 00:07:01,580
implementation is to think very

134
00:06:59,450 --> 00:07:03,500
systematic and carefully about the

135
00:07:01,580 --> 00:07:05,300
matrix dimensions you're working with so

136
00:07:03,500 --> 00:07:07,280
when I'm trying to develop my own code I

137
00:07:05,300 --> 00:07:08,960
often pull a piece of paper and just

138
00:07:07,280 --> 00:07:11,480
think carefully through so the

139
00:07:08,960 --> 00:07:13,940
dimensions of the matrix I'm working

140
00:07:11,480 --> 00:07:16,570
with let's see how you could do that in

141
00:07:13,940 --> 00:07:16,570
the next video