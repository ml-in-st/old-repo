1
00:00:00,000 --> 00:00:03,339
We've all been hearing that deep
neural networks work really well for

2
00:00:03,339 --> 00:00:07,073
a lot of problems, and it's not just that
they need to be big neural networks,

3
00:00:07,073 --> 00:00:10,718
is that specifically, they need to be
deep or to have a lot of hidden layers.

4
00:00:10,718 --> 00:00:12,208
So why is that?

5
00:00:12,208 --> 00:00:15,833
Let's go through a couple examples and
try to gain some intuition for

6
00:00:15,833 --> 00:00:17,720
why deep networks might work well.

7
00:00:17,720 --> 00:00:22,181
So first,
what is a deep network computing?

8
00:00:22,181 --> 00:00:25,393
If you're building a system for
face recognition or

9
00:00:25,393 --> 00:00:29,631
face detection, here's what a deep
neural network could be doing.

10
00:00:29,631 --> 00:00:35,059
Perhaps you input a picture of a face then
the first layer of the neural network

11
00:00:35,059 --> 00:00:40,000
you can think of as maybe being
a feature detector or an edge detector.

12
00:00:40,000 --> 00:00:45,519
In this example, I'm plotting what a
neural network with maybe 20 hidden units,

13
00:00:45,519 --> 00:00:48,017
might be kind of compute on this image.

14
00:00:48,017 --> 00:00:52,357
So the 20 hidden units visualize
by these little square boxes.

15
00:00:52,357 --> 00:00:57,325
So for example, this little visualization
represents a hidden unit is

16
00:00:57,325 --> 00:01:01,978
trying to figure out if where
the edges of that orientation in DMH.

17
00:01:01,978 --> 00:01:05,914
And maybe this hidden unit
maybe trying to figure

18
00:01:05,914 --> 00:01:09,955
out where are the horizontal
edges in this image.

19
00:01:09,955 --> 00:01:13,184
And when we talk about convolutional
networks in a later course,

20
00:01:13,184 --> 00:01:16,129
this particular visualization
will make a bit more sense.

21
00:01:16,129 --> 00:01:19,562
But the form, you can think of the first
layer of the neural network as look at

22
00:01:19,562 --> 00:01:22,690
the picture and try to figure out
where are the edges in this picture.

23
00:01:22,690 --> 00:01:27,356
Now, let's think about where the edges
in this picture by grouping together

24
00:01:27,356 --> 00:01:28,730
pixels to form edges.

25
00:01:28,730 --> 00:01:34,670
It can then de-detect the edges and group
edges together to form parts of faces.

26
00:01:34,670 --> 00:01:40,289
So for example, you might have a low
neuron trying to see if it's finding an I,

27
00:01:40,289 --> 00:01:44,480
or a different neuron trying
to find that part of the nose.

28
00:01:44,480 --> 00:01:47,463
And so by putting together lots of edges,

29
00:01:47,463 --> 00:01:50,970
it can start to detect
different parts of faces.

30
00:01:50,970 --> 00:01:56,035
And then, finally, by putting
together different parts of faces,

31
00:01:56,035 --> 00:02:01,006
like an eye or a nose or an ear or
a chin, it can then try to recognize or

32
00:02:01,006 --> 00:02:03,564
detect different types of faces.

33
00:02:03,564 --> 00:02:07,755
So intuitively, you can think of the
earlier layers of the neural network as

34
00:02:07,755 --> 00:02:10,190
detecting simple functions, like edges.

35
00:02:10,190 --> 00:02:14,573
And then composing them together in
the later layers of a neural network so

36
00:02:14,573 --> 00:02:17,625
that it can learn more and
more complex functions.

37
00:02:17,625 --> 00:02:23,640
These visualizations will make more sense
when we talk about convolutional nets.

38
00:02:23,640 --> 00:02:26,203
And one technical detail
of this visualization,

39
00:02:26,203 --> 00:02:29,802
the edge detectors are looking in
relatively small areas of an image,

40
00:02:29,802 --> 00:02:31,703
maybe very small regions like that.

41
00:02:31,703 --> 00:02:36,616
And then the facial detectors you can look
at maybe much larger areas of image but

42
00:02:36,616 --> 00:02:41,308
the main addition while you take away
from this is just finding simple things

43
00:02:41,308 --> 00:02:43,675
like edges and then building them up.

44
00:02:43,675 --> 00:02:47,216
Composing them together to detect
more complex things like an iron and

45
00:02:47,216 --> 00:02:50,530
then composing those together to
find more even complex things.

46
00:02:50,530 --> 00:02:55,665
And this type of simple to complex
hierarchical representation,

47
00:02:55,665 --> 00:02:58,508
or compositional representation,

48
00:02:58,508 --> 00:03:04,114
applies in other types of data than
images and face recognition as well.

49
00:03:04,114 --> 00:03:08,593
For example, if you're trying to
build a speech recognition system,

50
00:03:08,593 --> 00:03:10,908
it's hard to revisualize speech but

51
00:03:10,908 --> 00:03:15,684
if you input an audio clip then maybe
the first level of a neural network might

52
00:03:15,684 --> 00:03:20,863
learnt to detect low level audio wave form
features, such as is this toe going up?

53
00:03:20,863 --> 00:03:21,703
Is it going down?

54
00:03:21,703 --> 00:03:26,869
Is it white noise or
slithering sound like [SOUND].

55
00:03:26,869 --> 00:03:27,903
And what is the pitch?

56
00:03:27,903 --> 00:03:31,124
When it comes to that, detect low
level wave form features like that.

57
00:03:31,124 --> 00:03:34,233
And then by composing
low level wave forms,

58
00:03:34,233 --> 00:03:37,937
maybe you'll learn to detect
basic units of sound.

59
00:03:37,937 --> 00:03:40,297
In linguistics they call phonemes.

60
00:03:40,297 --> 00:03:45,098
But, for example, in the word cat,
the C is a phoneme, the A is a phoneme,

61
00:03:45,098 --> 00:03:46,787
the T is another phoneme.

62
00:03:46,787 --> 00:03:49,987
But learns to find maybe
the basic units of sound and

63
00:03:49,987 --> 00:03:54,688
then composing that together maybe
learn to recognize words in the audio.

64
00:03:54,688 --> 00:03:58,270
And then maybe compose those together,

65
00:03:58,270 --> 00:04:02,912
in order to recognize entire phrases or
sentences.

66
00:04:02,912 --> 00:04:07,572
So deep Internet work with multiple hidden
layers might be able to have the earlier

67
00:04:07,572 --> 00:04:10,477
layers learn these lower
level simple features and

68
00:04:10,477 --> 00:04:15,339
then have the later deeper layers then put
together the simpler things it's detected

69
00:04:15,339 --> 00:04:19,392
in order to detect more complex things
like recognize specific words or

70
00:04:19,392 --> 00:04:21,040
even phrases or sentences.

71
00:04:21,040 --> 00:04:24,745
The uttering in order to
carry out speech recognition.

72
00:04:24,745 --> 00:04:30,168
And what we see is that whereas the other
layers are computing, what seems like

73
00:04:30,168 --> 00:04:35,673
relatively simple functions of the input
such as right at the edges, by the time

74
00:04:35,673 --> 00:04:41,046
you get deep in the network you can
actually do surprisingly complex things.

75
00:04:41,046 --> 00:04:44,876
Such as detect faces or
detect words or phrases or sentences.

76
00:04:44,876 --> 00:04:48,767
Some people like to make an analogy
between deep neural networks and

77
00:04:48,767 --> 00:04:52,656
the human brain, where we believe,
or neuroscientists believe,

78
00:04:52,656 --> 00:04:57,162
that the human brain also starts off
detecting simple things like edges in what

79
00:04:57,162 --> 00:05:00,370
your eyes see then builds those
up to detect more complex

80
00:05:00,370 --> 00:05:02,440
things like the faces that you see.

81
00:05:02,440 --> 00:05:05,038
I think analogies between
deep learning and

82
00:05:05,038 --> 00:05:08,276
the human brain are sometimes
a little bit dangerous.

83
00:05:08,276 --> 00:05:13,301
But there is a lot of truth to, this being
how we think that human brain works and

84
00:05:13,301 --> 00:05:18,102
that the human brain probably
detects simple things like edges and

85
00:05:18,102 --> 00:05:22,598
then put them together to from more and
more complex objects and so that

86
00:05:22,598 --> 00:05:27,430
has served as a loose form of inspiration
for some people learning as well.

87
00:05:27,430 --> 00:05:29,850
We'll see a bit more
about the human brain or

88
00:05:29,850 --> 00:05:33,065
about the biological brain in
the lead of video this week.

89
00:05:35,534 --> 00:05:40,407
The other piece of intuition
about why deep networks seem to

90
00:05:40,407 --> 00:05:42,756
work well is the following.

91
00:05:42,756 --> 00:05:47,868
So this result comes from circuit
theory of which pertains the thinking

92
00:05:47,868 --> 00:05:53,760
about what types of functions you can
compute with different logic case.

93
00:05:53,760 --> 00:05:58,860
So informally, their functions compute
with a relatively small but deep neural

94
00:05:58,860 --> 00:06:03,595
network and by small I mean the number
of hidden units is relatively small.

95
00:06:03,595 --> 00:06:07,553
But if you try to compute the same
function with a shallow network,

96
00:06:07,553 --> 00:06:09,178
so hidden layers,

97
00:06:09,178 --> 00:06:13,296
then you might require exponentially
more hidden units to compute.

98
00:06:13,296 --> 00:06:18,109
So let me just give you one example and
illustrate this a bit informally.

99
00:06:18,109 --> 00:06:21,423
But let's say you're trying to
compute the exclusive [INAUDIBLE] or

100
00:06:21,423 --> 00:06:23,349
the parity of all your input features.

101
00:06:23,349 --> 00:06:28,430
So you're trying to compute X1,
XOR, X2, XOR,

102
00:06:28,430 --> 00:06:33,064
X3, XOR, up to Xn if you have n or
n X features.

103
00:06:33,064 --> 00:06:39,924
So if you build in XOR free like this,
so for us it computes the XOR of X1 and

104
00:06:39,924 --> 00:06:44,586
X2, then take X3 and
X4 and compute their XOR.

105
00:06:44,586 --> 00:06:49,392
And technically, if you're just using
ends or not gauge, you might need

106
00:06:49,392 --> 00:06:54,196
couple layers to compute the XOR
function rather than just one layer, but

107
00:06:54,196 --> 00:06:58,791
with a relatively small circuit,
you can compute the XOR, and so on.

108
00:06:58,791 --> 00:07:03,987
And then you can build,
really, an XOR tree like so,

109
00:07:03,987 --> 00:07:12,090
until eventually, you have a circuit here
that outputs, well, lets call this Y.

110
00:07:12,090 --> 00:07:15,236
The outputs of Y hat equals Y.

111
00:07:15,236 --> 00:07:18,398
The exclusive or
the parity of all these input bits.

112
00:07:18,398 --> 00:07:24,790
So to compute XOR, the definite left
network will be on the order of log N.

113
00:07:24,790 --> 00:07:27,410
We'll just have an XOR tree.

114
00:07:27,410 --> 00:07:30,836
So the number of nodes or
the number of circuit components or

115
00:07:30,836 --> 00:07:33,929
the number of gates in this
network is not that large.

116
00:07:33,929 --> 00:07:38,452
You don't need that many gates in
order to compute the exclusive OR.

117
00:07:38,452 --> 00:07:43,458
But now, if you are not allowed to
use a neural network with north pole

118
00:07:43,458 --> 00:07:48,203
hidden layers with, in this case,
order log and hidden layers,

119
00:07:48,203 --> 00:07:53,382
if you're forced to compute this
function with just one hidden layer,

120
00:07:53,382 --> 00:07:57,912
so you have all these things going into,
so the hidden units.

121
00:07:57,912 --> 00:08:02,116
And then these things then output Y.

122
00:08:02,116 --> 00:08:07,120
Then in order to compute this
XOR function, this hidden layer

123
00:08:07,120 --> 00:08:12,124
will need to be exponentially large,
because essentially,

124
00:08:12,124 --> 00:08:18,397
you need to exhaustively enumerate our
2 to the N possible configurations.

125
00:08:18,397 --> 00:08:23,139
So on the order of 2 to the N,
possible configurations of the input

126
00:08:23,139 --> 00:08:27,898
bits that result in the exclusive
[INAUDIBLE] being either 1 or 0.

127
00:08:27,898 --> 00:08:32,213
So you end up needing a hidden layer
that is exponentially large in

128
00:08:32,213 --> 00:08:33,554
the number of bits.

129
00:08:33,554 --> 00:08:38,229
I think technically, you could do this
with 2 to the N minus 1 hidden units.

130
00:08:38,229 --> 00:08:43,948
But that's the older 2 to the N processes
explanation larger the number of bits.

131
00:08:43,948 --> 00:08:49,149
So I hope this gives a sense that
there are mathematical functions,

132
00:08:49,149 --> 00:08:55,275
that are much easier to compute with deep
networks than with shallow networks.

133
00:08:55,275 --> 00:09:01,028
Actually, I personally found the result
from circuit theory less useful for

134
00:09:01,028 --> 00:09:05,985
gaining intuitions, but
just one of the results that people often

135
00:09:05,985 --> 00:09:11,223
cite when explaining the value of
having very deep representations.

136
00:09:11,223 --> 00:09:13,600
Now, in addition to this reasons for

137
00:09:13,600 --> 00:09:16,897
preferring deep neural
networks to be roughly on,

138
00:09:16,897 --> 00:09:22,204
is I think the other reasons the term deep
learning has taken off is just branding.

139
00:09:22,204 --> 00:09:26,776
This things just we call neural
networks belong to hidden layers, but

140
00:09:26,776 --> 00:09:31,198
the phrase deep learning is just
a great brand, it's just so deep.

141
00:09:31,198 --> 00:09:36,284
So I think that once that term caught on
that really new networks rebranded or

142
00:09:36,284 --> 00:09:39,622
new networks with many
hidden layers rebranded,

143
00:09:39,622 --> 00:09:42,970
help to capture the popular
imagination as well.

144
00:09:42,970 --> 00:09:47,479
They regard as the PR branding
deep networks do work well.

145
00:09:47,479 --> 00:09:51,342
Sometimes people go overboard and
insist on using tons of hidden layers.

146
00:09:51,342 --> 00:09:55,500
But when I'm starting out a new problem,
I'll often really start out with

147
00:09:55,500 --> 00:09:58,803
neuro-logistic regression then
try something with one or

148
00:09:58,803 --> 00:10:01,722
two hidden layers and
use that as a hyper parameter.

149
00:10:01,722 --> 00:10:05,731
Use that as a parameter or hyper parameter
that you tune in order to try to find

150
00:10:05,731 --> 00:10:07,935
the right depth for your neural network.

151
00:10:07,935 --> 00:10:12,800
But over the last several years there has
been a trend toward people finding that

152
00:10:12,800 --> 00:10:17,590
for some applications, very, very deep
neural networks here with maybe many

153
00:10:17,590 --> 00:10:22,264
dozens of layers sometimes, can sometimes
be the best model for a problem.

154
00:10:22,264 --> 00:10:27,605
So that's it for the intuitions for
why deep learning seems to work well.

155
00:10:27,605 --> 00:10:31,411
Let's now take a look at the mechanics
of how to implement not just front

156
00:10:31,411 --> 00:10:33,769
propagation, but also back propagation.