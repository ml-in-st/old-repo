1
00:00:00,030 --> 00:00:04,440
in the last video you saw what a single

2
00:00:02,370 --> 00:00:06,270
hidden layer neural network looks like

3
00:00:04,440 --> 00:00:08,309
in this video let's go through the

4
00:00:06,270 --> 00:00:10,710
details of exactly how this neural

5
00:00:08,309 --> 00:00:13,860
network computers outputs what you see

6
00:00:10,710 --> 00:00:15,960
is that is like logistic regression the

7
00:00:13,860 --> 00:00:18,390
repeater of all the times let's take a

8
00:00:15,960 --> 00:00:19,350
look so this is what's a two layer

9
00:00:18,390 --> 00:00:22,109
neural network

10
00:00:19,350 --> 00:00:23,130
let's go more deeply into exactly what

11
00:00:22,109 --> 00:00:25,380
this neural network

12
00:00:23,130 --> 00:00:28,140
compute now was said before that

13
00:00:25,380 --> 00:00:30,000
logistic regression the circle images

14
00:00:28,140 --> 00:00:32,189
the regression really represents two

15
00:00:30,000 --> 00:00:34,380
steps of computation first you compute Z

16
00:00:32,189 --> 00:00:37,440
as follows and in second you compute the

17
00:00:34,380 --> 00:00:39,870
activation as a sigmoid function of Z so

18
00:00:37,440 --> 00:00:42,090
a neural network just does this a lot

19
00:00:39,870 --> 00:00:43,890
more times let's start by focusing on

20
00:00:42,090 --> 00:00:45,629
just one of the nodes in the hidden

21
00:00:43,890 --> 00:00:47,160
there and this look at the first node in

22
00:00:45,629 --> 00:00:48,870
the hidden layer so I've grayed out the

23
00:00:47,160 --> 00:00:50,820
other nodes for now so similar to

24
00:00:48,870 --> 00:00:53,489
logistic regression on the left this

25
00:00:50,820 --> 00:00:55,379
node in a hidden layer does two steps of

26
00:00:53,489 --> 00:00:57,629
computation right the first step and

27
00:00:55,379 --> 00:01:01,649
think it's as the left half of this node

28
00:00:57,629 --> 00:01:05,339
it computes Z equals W transpose X plus

29
00:01:01,649 --> 00:01:07,290
B and the notation we'll use is um these

30
00:01:05,339 --> 00:01:09,030
are all quantities associated with the

31
00:01:07,290 --> 00:01:10,619
first hidden layer so that's why we have

32
00:01:09,030 --> 00:01:13,170
a bunch of square brackets there and

33
00:01:10,619 --> 00:01:14,850
this is the first node in the hidden

34
00:01:13,170 --> 00:01:17,310
layer so that's why we have the

35
00:01:14,850 --> 00:01:19,409
subscript one over there so first it

36
00:01:17,310 --> 00:01:23,400
does that and then a second step is it

37
00:01:19,409 --> 00:01:26,100
computes a 1 1 equals sigmoid of z11

38
00:01:23,400 --> 00:01:29,970
like so so for both Z and ay the

39
00:01:26,100 --> 00:01:32,159
notational convention is that a Li the L

40
00:01:29,970 --> 00:01:33,840
here in superscript square brackets

41
00:01:32,159 --> 00:01:36,960
refers to layer number and the I

42
00:01:33,840 --> 00:01:38,820
subscript here refers to the nodes in

43
00:01:36,960 --> 00:01:40,890
that layer so then they will be looking

44
00:01:38,820 --> 00:01:43,560
at is layer 1 that is a hidden layer

45
00:01:40,890 --> 00:01:46,439
node 1 so that's why the superscript and

46
00:01:43,560 --> 00:01:48,689
subscript were on both 1 1 so that

47
00:01:46,439 --> 00:01:51,149
little circle that first node in your

48
00:01:48,689 --> 00:01:53,220
network represents carrying out these

49
00:01:51,149 --> 00:01:55,320
two steps of computation now let's look

50
00:01:53,220 --> 00:01:57,299
at the second node in your network the

51
00:01:55,320 --> 00:01:59,790
second node in the hidden layer of in

52
00:01:57,299 --> 00:02:01,979
your network similar to the logistic

53
00:01:59,790 --> 00:02:03,930
regression unit on the left this little

54
00:02:01,979 --> 00:02:06,030
circle represents two steps of

55
00:02:03,930 --> 00:02:08,520
computation the first step is a

56
00:02:06,030 --> 00:02:10,920
confusing Z this is still layer 1

57
00:02:08,520 --> 00:02:13,470
pronounced the second node equals W

58
00:02:10,920 --> 00:02:18,930
transpose X plus V

59
00:02:13,470 --> 00:02:20,640
- and then a 1/2 equals Sigma z12 and

60
00:02:18,930 --> 00:02:22,170
again feel free to pause the video if

61
00:02:20,640 --> 00:02:24,360
you want but you can double check that

62
00:02:22,170 --> 00:02:26,730
the superscript and subscript notation

63
00:02:24,360 --> 00:02:28,980
is consistent with what we have written

64
00:02:26,730 --> 00:02:31,650
here above in purple so we'll talk

65
00:02:28,980 --> 00:02:33,870
through the first two hidden units in

66
00:02:31,650 --> 00:02:35,700
the neural network on hidden units three

67
00:02:33,870 --> 00:02:38,640
and four also represents some

68
00:02:35,700 --> 00:02:41,580
computations so now let me take this

69
00:02:38,640 --> 00:02:43,410
pair of equations and this pair of

70
00:02:41,580 --> 00:02:45,420
equations and let's copy them to the

71
00:02:43,410 --> 00:02:47,880
moon fly so here's our network and

72
00:02:45,420 --> 00:02:49,920
here's the first and there's a second

73
00:02:47,880 --> 00:02:53,400
equations they were worked on previously

74
00:02:49,920 --> 00:02:55,560
for the first and the second hidden

75
00:02:53,400 --> 00:02:57,840
units if you then go through and write

76
00:02:55,560 --> 00:03:00,360
out the corresponding equations for the

77
00:02:57,840 --> 00:03:02,460
third and fourth hidden units you get

78
00:03:00,360 --> 00:03:06,150
the following and those make sure this

79
00:03:02,460 --> 00:03:09,450
notation is clear this is the vector W 1

80
00:03:06,150 --> 00:03:11,070
1 this is a vector transpose x I think

81
00:03:09,450 --> 00:03:13,140
so that's what the superscript G there

82
00:03:11,070 --> 00:03:15,480
represents this is a vector transpose

83
00:03:13,140 --> 00:03:16,920
now as you might have guessed if you're

84
00:03:15,480 --> 00:03:19,410
actually implementing in your network

85
00:03:16,920 --> 00:03:21,630
doing this with a for loop seems really

86
00:03:19,410 --> 00:03:24,989
inefficient so what we're going to do is

87
00:03:21,630 --> 00:03:26,700
take these four equations and vectorize

88
00:03:24,989 --> 00:03:29,760
so I'm going to start by showing how to

89
00:03:26,700 --> 00:03:30,690
compute Z as a vector it turns out you

90
00:03:29,760 --> 00:03:34,080
could do it as follows

91
00:03:30,690 --> 00:03:37,739
let me take these WS and stack them into

92
00:03:34,080 --> 00:03:40,140
a matrix then you have W 1 1 transpose

93
00:03:37,739 --> 00:03:41,670
so that's a row vector of the column

94
00:03:40,140 --> 00:03:47,100
vector transpose gives you a row vector

95
00:03:41,670 --> 00:03:50,160
then W 1 2 transpose W 1 3 transpose of

96
00:03:47,100 --> 00:03:53,340
V 1 4 transpose and so this by stacking

97
00:03:50,160 --> 00:03:55,260
goes from for W vectors together you end

98
00:03:53,340 --> 00:03:57,420
up with a matrix so another way to think

99
00:03:55,260 --> 00:03:59,670
of this is that we have for logistic

100
00:03:57,420 --> 00:04:01,440
regression unions there and each of the

101
00:03:59,670 --> 00:04:04,140
logistic regression unions have a

102
00:04:01,440 --> 00:04:06,150
corresponding parameter vector W and by

103
00:04:04,140 --> 00:04:09,180
stacking those four vectors together you

104
00:04:06,150 --> 00:04:11,310
end up with this four by three matrix so

105
00:04:09,180 --> 00:04:14,160
if you then take this matrix and

106
00:04:11,310 --> 00:04:17,700
multiply it by your input features x1 x2

107
00:04:14,160 --> 00:04:19,680
x3 you end up with by our matrix

108
00:04:17,700 --> 00:04:24,990
multiplication works you end up with w1

109
00:04:19,680 --> 00:04:26,260
1 transpose x w1 w2 1 transpose X of U 3

110
00:04:24,990 --> 00:04:29,800
1 transyl

111
00:04:26,260 --> 00:04:32,620
XW 1 transpose X and then let's not

112
00:04:29,800 --> 00:04:33,340
forget the bees so we now add to this a

113
00:04:32,620 --> 00:04:40,450
vector

114
00:04:33,340 --> 00:04:46,630
b11 b12 b13 in 1/4 so that they see this

115
00:04:40,450 --> 00:04:48,880
then this is b11 b12 b13 e 1/4 and so

116
00:04:46,630 --> 00:04:51,520
you see that each of the four rows of

117
00:04:48,880 --> 00:04:53,860
this outcome correspond exactly to each

118
00:04:51,520 --> 00:04:56,260
of these four rows of each these four

119
00:04:53,860 --> 00:04:58,090
quantities that we had above so in other

120
00:04:56,260 --> 00:05:02,890
words we've just shown that this thing

121
00:04:58,090 --> 00:05:05,950
is therefore equal to V 1 1 V 1 to V 1 V

122
00:05:02,890 --> 00:05:07,630
V 1 4 right as defined here and maybe

123
00:05:05,950 --> 00:05:10,060
not surprisingly we're going to call

124
00:05:07,630 --> 00:05:11,890
this whole thing the vector V 1 which is

125
00:05:10,060 --> 00:05:14,560
taken by stacking up these um

126
00:05:11,890 --> 00:05:16,810
individuals of these into a column

127
00:05:14,560 --> 00:05:19,360
vector when we're vectorizing one of the

128
00:05:16,810 --> 00:05:21,070
rules of thumb that might help you

129
00:05:19,360 --> 00:05:23,140
navigate this is that when we have

130
00:05:21,070 --> 00:05:25,420
different nodes in a layer or stack them

131
00:05:23,140 --> 00:05:28,390
vertically so that's why when you have Z

132
00:05:25,420 --> 00:05:30,460
1 1 2 Z 1 for those correspond to four

133
00:05:28,390 --> 00:05:32,590
different nodes in the hidden layer and

134
00:05:30,460 --> 00:05:34,920
so we stack these four numbers

135
00:05:32,590 --> 00:05:37,810
vertically to form the vectors V 1 and

136
00:05:34,920 --> 00:05:40,150
reduce one more piece of notation this 4

137
00:05:37,810 --> 00:05:43,840
by 3 matrix here which we obtained by

138
00:05:40,150 --> 00:05:45,460
stacking the lower case you know W 1 1 W

139
00:05:43,840 --> 00:05:49,060
1 2 and so on we're going to call this

140
00:05:45,460 --> 00:05:51,760
matrix W capital 1 and similarly this

141
00:05:49,060 --> 00:05:53,890
vector or going to call B superscript 1

142
00:05:51,760 --> 00:05:56,740
square bracket and so this is a 4 by 1

143
00:05:53,890 --> 00:05:59,920
vector so now we've computed Z using

144
00:05:56,740 --> 00:06:01,750
this vector matrix notation the last

145
00:05:59,920 --> 00:06:04,240
thing we need to do is also compute

146
00:06:01,750 --> 00:06:06,340
these values of a and so probably won't

147
00:06:04,240 --> 00:06:09,400
surprise you to see that we're going to

148
00:06:06,340 --> 00:06:13,240
define a 1 as just stacking together

149
00:06:09,400 --> 00:06:15,190
those activation values a11 to a14 so

150
00:06:13,240 --> 00:06:18,310
just take these 4 values and stack them

151
00:06:15,190 --> 00:06:21,190
together in a vector called a1 and this

152
00:06:18,310 --> 00:06:23,380
is going to be sigmoid of z1 where

153
00:06:21,190 --> 00:06:25,750
there's no husband implementation of the

154
00:06:23,380 --> 00:06:27,940
sigmoid function that takes in the four

155
00:06:25,750 --> 00:06:31,180
elements of Z and applies the sigmoid

156
00:06:27,940 --> 00:06:35,020
function element wise to it so just a

157
00:06:31,180 --> 00:06:38,230
recap we figured out that z1 is equal to

158
00:06:35,020 --> 00:06:39,559
W 1 times the vector X plus the vector B

159
00:06:38,230 --> 00:06:43,069
1 and a 1

160
00:06:39,559 --> 00:06:44,779
is sigmoid x z 1 let's just copy this to

161
00:06:43,069 --> 00:06:46,429
the next slide and what we see is that

162
00:06:44,779 --> 00:06:49,219
for the first layer of the neural

163
00:06:46,429 --> 00:06:52,609
network given an input X we have that z1

164
00:06:49,219 --> 00:06:56,329
is equal to w1 times X plus B 1 and a 1

165
00:06:52,609 --> 00:06:59,899
is sick point of z1 and the dimensions

166
00:06:56,329 --> 00:07:04,129
of this are 4 by 1 equals this is a 4 by

167
00:06:59,899 --> 00:07:07,129
3 matrix times a 3 by 1 vector plus a 4

168
00:07:04,129 --> 00:07:10,309
by 1 vector B and this is 4 by 1 same

169
00:07:07,129 --> 00:07:13,909
dimensions and remember that we said X

170
00:07:10,309 --> 00:07:17,389
is equal to a 0 right just like Y hat is

171
00:07:13,909 --> 00:07:19,549
also equal to a 2 so if you want you can

172
00:07:17,389 --> 00:07:23,389
actually take this X and replace it with

173
00:07:19,549 --> 00:07:25,519
a 0 since a 0 is if you want as an alias

174
00:07:23,389 --> 00:07:27,529
for the vector of input futures X now

175
00:07:25,519 --> 00:07:30,199
through a similar derivation you can

176
00:07:27,529 --> 00:07:32,569
figure out that the representation for

177
00:07:30,199 --> 00:07:34,999
the next layer can also be written

178
00:07:32,569 --> 00:07:38,389
similarly where what the output layer

179
00:07:34,999 --> 00:07:41,899
does is it has associated with it so the

180
00:07:38,389 --> 00:07:44,689
parameters W 2 and B 2 so W 2 in this

181
00:07:41,899 --> 00:07:46,999
case is going to be a 1 by 4 matrix and

182
00:07:44,689 --> 00:07:49,969
B 2 is just a real number as 1 by 1 and

183
00:07:46,999 --> 00:07:52,549
so V 2 is going to be a real numbers

184
00:07:49,969 --> 00:07:56,449
right as a 1 by 1 matrix is going to be

185
00:07:52,549 --> 00:07:59,059
a 1 by 4 thing times a was 4 by 1 plus B

186
00:07:56,449 --> 00:08:00,619
2 is 1 by 1 and so this gives you just a

187
00:07:59,059 --> 00:08:03,349
real number and if you think of this

188
00:08:00,619 --> 00:08:04,969
loss output unit as just being analogous

189
00:08:03,349 --> 00:08:09,499
to logistic regression which had

190
00:08:04,969 --> 00:08:13,219
parameters W and B on W really plays in

191
00:08:09,499 --> 00:08:16,279
nablus role to W 2 transpose or W 2's

192
00:08:13,219 --> 00:08:18,709
really W transpose and B is equal to B 2

193
00:08:16,279 --> 00:08:21,199
right similar to you know cover up the

194
00:08:18,709 --> 00:08:23,299
left of this network and ignore all that

195
00:08:21,199 --> 00:08:25,339
for now then this is just this last

196
00:08:23,299 --> 00:08:27,289
output unit there's a lot like logistic

197
00:08:25,339 --> 00:08:29,599
regression except that instead of

198
00:08:27,289 --> 00:08:32,779
writing the parameters as WMV we're

199
00:08:29,599 --> 00:08:36,079
writing them as W 2 and B 2 with

200
00:08:32,779 --> 00:08:38,809
dimensions one by four and one by one so

201
00:08:36,079 --> 00:08:41,059
just a recap for logistic regression to

202
00:08:38,809 --> 00:08:43,519
implement the output or the influence

203
00:08:41,059 --> 00:08:47,809
prediction you compute Z equals W

204
00:08:43,519 --> 00:08:50,790
transpose X plus B and a y hat equals a

205
00:08:47,809 --> 00:08:53,520
equals sigmoid of z

206
00:08:50,790 --> 00:08:55,170
when you have a new network who have one

207
00:08:53,520 --> 00:08:58,050
fit in there what you need to implement

208
00:08:55,170 --> 00:09:00,270
two computers output is just the four

209
00:08:58,050 --> 00:09:02,880
equation and you can think of this as a

210
00:09:00,270 --> 00:09:05,700
vectorized implementation of computing

211
00:09:02,880 --> 00:09:07,530
the output of first these four

212
00:09:05,700 --> 00:09:09,420
logistical russian units and hitting

213
00:09:07,530 --> 00:09:12,060
there that's what this does and then

214
00:09:09,420 --> 00:09:13,650
this which is regression in the output

215
00:09:12,060 --> 00:09:15,720
layer which is what this does

216
00:09:13,650 --> 00:09:18,120
I hope this description made sense but

217
00:09:15,720 --> 00:09:20,010
takeaway is to compute the output of

218
00:09:18,120 --> 00:09:22,500
this neural network all you need is

219
00:09:20,010 --> 00:09:25,050
those four lines of code so now you've

220
00:09:22,500 --> 00:09:27,360
seen how given a single input feature

221
00:09:25,050 --> 00:09:29,250
vector at you can with four lines of

222
00:09:27,360 --> 00:09:31,590
code compute the outputs of this viewer

223
00:09:29,250 --> 00:09:33,390
Network um similar to what we did for

224
00:09:31,590 --> 00:09:35,550
logistic regression will also want to

225
00:09:33,390 --> 00:09:38,580
vectorize across multiple training

226
00:09:35,550 --> 00:09:40,710
examples and we'll see that by stacking

227
00:09:38,580 --> 00:09:42,330
up training examples in different colors

228
00:09:40,710 --> 00:09:44,970
in the matrix or just slight

229
00:09:42,330 --> 00:09:46,830
modification to this you also similar to

230
00:09:44,970 --> 00:09:49,350
what you saw in which is regression be

231
00:09:46,830 --> 00:09:51,240
able to compute the output of this

232
00:09:49,350 --> 00:09:53,220
neural network not just on one example

233
00:09:51,240 --> 00:09:55,770
at a time belong your say your

234
00:09:53,220 --> 00:09:59,480
anti-trade set at a time so let's see

235
00:09:55,770 --> 00:09:59,480
the details of that in the next video