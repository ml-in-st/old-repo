בסרטונים קודמים, דברנו על אלגוריתם הירידה במדרון ודברנו על מודל הרגרסיה הליניארית ועל פונקצית העלות של ריבועי השגיאות. בסרטון הזה אנחנו נחבר את הירידה במדרון עם פונקציית העלות וזה ייתן לנו את אלגוריתם הרגרסיה הליניארית או את הקו הישר העובר דרך הנתונים שלנו. אז לאן הגענו בסוף קטעי הווידאו הקודמים. אלגוריתם הירידה במדרון כבר אמור להיות מוכר לכם, והנה מודל הרגרסיה הליניארית עם פונקציית ההשערה הליניארית ופונקצית העלות של ריבוע השגיאות. מה שאנחנו נעשה עכשיו הוא להחיל את הירידה במדרון כדי למזער את פונקצית העלות הזו. כדי להחיל את הירידה במדרון, לממש את קטע הקוד הזה, הביטוי המרכזי שאנחנו צריכים הוא הנגזרת הזו כאן. אז צריך להבין מה היא הנגזרת החלקית הזו, והצבה של ההגדרה של פונקצית העלות J נותנת את הדבר הזה. סכום מ-i שווה 1 עד m. של ריבוע השגיאה של האיבר ה-i בפונקצית העלות. וכל מה שעשיתי כאן היה פשוט להציב כאן את ההגדרה של פונקצית העלות. וכשנפשט את זה עוד קצת, זה יצא שווה לזה. הסכום מאחד עד m של θ₀ פלוס θ₁ כפול xi מינוס yi בריבוע. וכל מה שעשיתי היה לקחת את הגדרת פונקצית ההשערה שלי ולהציב את זה שם. אז אנחנו צריכים לחשב מה היא הנגזרת החלקית הזו עבור שני מקרים, j=0 ו-j=1. אנחנו רוצים לדעת את הנגזרת החלקית ל-θ₀ ו-θ₁. ואני פשוט אכתוב את התשובות. מתברר שהביטוי הראשון הוא 1 חלקי m כפול הסכום של השגיאות של כל סדרת האימון, (h(x(i))-y(i ועבור הנגזרת החלקית הזו של θ₁, נקבל את הביטוי הזה. (h(x(i))-y(i כפול (x(i. כשמחשבים את הנגזרות החלקיות האלה, אנחנו הולכים מהמשוואה הזו אל כל אחת מהמשוואות האלה. החישוב של הנגזרות החלקיות האלה דורש קצת חשבון דיפרנציאלי. אם אתה יודע חשבון דיפרנציאלי, תרגיש חופשי לנסות בעצמך ולראות שאם אתה מחשב, אתה מקבל אותן התשובות שאני קיבלתי. אבל אם אתה פחות מכיר חשבון דיפרנציאלי, אל תדאג וזה בסדר פשוט להשתמש במשוואות האלה שיצאו לנו, לא חייבים לדעת חשבון דיפרנציאלי כדי לעשות את שיעורי הבית אז בואו פשוט ניישם את הירידה במדרון ונחזור לעבודה. אז חמושים בהגדרות אלה ובנגזרות שחישבנו, שהם בעצם פשוט השיפוע של פונקצית העלות J עכשיו אנחנו יכולים להציב אותם בחזרה לתוך אלגוריתם הירידה במדרון שלנו. אז הנה הירידה במדרון עבור רגרסיה ליניארית, אותה מריצים חזור והרץ עד להתכנסות, מפחיתים מ-θ₀ ו-θ₁ פחות α כפול הנגזרת הזו, הביטוי הזה כאן. אז הנה אלגוריתם הרגרסיה הליניארית שלנו. הביטוי הראשון הזה שכאן. הביטוי הזה הוא כמובן פשוט הנגזרת החלקית לפי θ₀, שחישבנו בשקופית קודמת. וגם הביטוי השני הזה כאן, הביטוי הזה הוא פשוט הנגזרת החלקית לפי θ₁, שגם אותה חישבנו בשקופית הקודמת. ולתזכורת, כשמיישמים ירידה במדרון חייבים לזכור את הפרט הזה שצריך לבנות את היישום כך שיעדכן את θ₀ ו-θ₁ בו-זמנית. אז בואו נראה איך עובדת הירידה במדרון. אחת הסוגיות שראינו עם הירידה במדרון היא שהוא עשוי להיות רגיש לאופטימום מקומי. כשהסברתי את הירידה במדרון בהתחלה הראיתי את התמונה הזאת של איך הוא מתגלגל בירידה על פני השטח, וראינו שלפי המיקום של הנקודה הראשונית, אפשר להגיע בסוף למינימום מקומי שונה. נגיע או לכאן או לכאן. אבל מתברר שפונקציית העלות עבור רגרסיה ליניארית היא תמיד פונקציה כזו בצורת קערה. המונח הטכני לכך הוא שזוהי פונקציה קמורה. ואני לא נותן כאן הגדרה פורמלית של מה זו פונקציה קמורה, C, O, N, V, E, X. אבל באופן לא פורמלי, פונקציה קמורה פרושה פונקציה בצורת קערה ולפונקציה כזו אין שום נקודות אופטימום מקומיות למעט האופטימום הגלובלי האחד. ולכן הירידה במדרון על סוג כזה של פונקציית עלות שהיא הדבר היחיד שיכול להיות ברגרסיה ליניארית תמיד תתכנס לאופטימום הגלובלי. כי אין לה שום אופטימום אחר, גלובלי או מקומי. אז עכשיו בואו נראה את האלגוריתם הזה בפעולה. כרגיל, כאן ציירתי את פונקצית ההשערה ואת פונקצית העלות J. ונניח שאתחלתי את הפרמטרים שלי בערכים האלה. בדרך כלל מאתחלים את הפרמטרים באפס אפס. θ₀ ו-θ₁ שניהם שווים אפס. אבל לשם ההדגמה אתחלתי אותם כך, θ₀ שווה 900 ו-θ₁ בסביבות 0.1-, בסדר. הפונקציה מתאימה לקו h(x)=-900-0.1x, הקו הזה כאן על פונקציית העלות. עכשיו, אם נעשה צעד אחד של הירידה במדרון, נגיע מהנקודה הזו כאן, כלפי למטה ושמאלה, לנקודה השנייה הזו שם. ואפשר לשים לב שהקו משתנה קצת, וכשעושים עוד צעד של הירידה במדרון, הקו מהשמאל שוב משתנה. נכון? וגם עברנו לנקודה חדשה על פונקצית העלות. וכשנמשיך לעשות צעדים בירידה במדרון, גם המחיר יורד. אז הפרמטרים עוברים דרך המסלול הזה. ואם תסתכלו על צד שמאל, גם פונקצית ההשערב עוברת אותו תהליך. ההתאמה שלה לנתונים הולכת ומשתפרת עד שלבסוף אנחנו מגיעים למינימום הגלובלי, המינימום הגלובלי הזה מתאים לפונקצית ההשערה הזו, שהיא מתאימה מאוד לנתונים. אז זהו הירידה במדרון, הפעלנו אותו וקבלנו התאמה טובה לסדרת הנתונים של מחירי הבתים. ועכשיו אפשר להשתמש בו כדי לחזות אם לחבר שלך יש בית בגודל 1250 רגל מרובעת, עכשיו אתה יכול לקרוא בגרף את הערך ולהגיד להם שהם יכולים לקבל אולי $250,000 עבור הבית שלהם. יש לאלגוריתם הזה גם שם אחר, מתברר שהאלגוריתם עליו עברנו כרגע נקרא לפעמים הירידה במדרון או בשיפוע באצְוָוה. מסתבר שבלמידה חישובית, אני מרגיש שאנשי למידה חישובית כמונו לא תמיד הכי מוצלחים במתן שמות לאלגוריתמים. אבל המונח הירידה בשיפוע באצְוָוה מתייחס לעובדה שבכל צעד של הירידה בשיפוע אנחנו מתחשבים בכל דוגמאות האימון. אז בירידה בשיפוע, בחישוב הנגזרת, אנחנו מחשבים את הסכומים האלה. אז בכל צעד של ירידה בשיפוע אנחנו מחשבים משהו כזה שמסכם את כל דוגמאות האימון שלנו ולכן המונח ירידה בשיפוע באצווה מתייחס לעובדה שאנחנו מסתכלים על כל האצווה של דוגמאות אימון. ושוב, זה ממש לא שם טוב, אבל כך קוראים לזה אנשי למידת מכונה. ומתברר שיש לפעמים גרסאות אחרות של ירידה במדרון שאינם גרסאות אצווה, אלא במקום זה לא מסתכלים על כל סדרת האימון אלא על תת סדרה קטנה של דוגמאות האימון בכל פעם. ונדבר גם על הגירסאות האלה בהמשך הקורס. אבל לעת עתה באמצעות האלגוריתם שלמדנו, ירידה בשיפוע באצְוָוה עכשיו אנחנו יודעים כיצד ליישם ירידה בשיפוע עבור רגרסיה ליניארית. אז זהו זה בקשר לרגרסיה ליניארית עם ירידה במדרון. אם כבר למדת אלגברה לינארית מתקדמת, חלק מכם אולי כבר עבר קורס של אלגברה לינארית מתקדמת. אז אתה אולי יודע שקיימת נוסחה לפתרון של המינימום של פונקצית העלות J ללא צורך בשימוש באלגוריתם איטרטיבי כמו ירידה בשיפוע. בהמשך הקורס נדבר גם על השיטה שמוצאת את המינימום של פונקציה העלות J ללא צורך בשלבים המרובים האלה של הירידה בשיפוע. השיטה האחרת הזו נקראת שיטת המשוואות הנורמליות. אבל אם במקרה שמעת על השיטה, מתברר כי שלירידה בשיפוע יש ביצועים עדיפים כאשר מדובר בערכות נתונים גדולות יותר מאשר שיטת המשוואות הנורמליות. ועכשיו שאנחנו יודעים על ירידה בשיפוע נוכל להשתמש בו בהקשרים רבים ושונים ואנחנו נשתמש בו גם בהרבה מאוד בעיות שונות של למידת מכונה. אז איחוליי על למידתכם על האלגוריתם הראשון שלכם בלמידה חישובית. מאוחר יותר נעשה תרגילים שבהם נבקש מכם ליישם את הירידה בשיפוע ובתקווה שתשתמשו באלגוריתמים האלה ממש בעצמכם. אבל קודם כל אני רוצה לספר לכם על הסדרה הבאה של קטעי וידאו. ובראש ובראשונה נספר לכם על הכללה של אלגוריתם הירידה במדרון שתעשה אותו הרבה יותר חזק. ואני מניח שאני אספר לכם על זה בסרטון הבא.