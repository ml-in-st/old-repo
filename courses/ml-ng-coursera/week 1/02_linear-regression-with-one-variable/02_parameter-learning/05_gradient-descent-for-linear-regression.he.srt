1
00:00:00,520 --> 00:00:04,480
בסרטונים קודמים, דברנו על אלגוריתם הירידה במדרון

2
00:00:04,480 --> 00:00:09,540
ודברנו על מודל הרגרסיה הליניארית ועל פונקצית העלות של ריבועי השגיאות.

3
00:00:09,540 --> 00:00:14,280
בסרטון הזה אנחנו נחבר את הירידה במדרון עם פונקציית העלות

4
00:00:14,280 --> 00:00:17,400
וזה ייתן לנו את אלגוריתם הרגרסיה הליניארית

5
00:00:17,400 --> 00:00:18,730
או את הקו הישר העובר דרך הנתונים שלנו.

6
00:00:20,800 --> 00:00:24,950
אז לאן הגענו בסוף קטעי הווידאו הקודמים.

7
00:00:24,950 --> 00:00:28,920
אלגוריתם הירידה במדרון כבר אמור להיות מוכר לכם,

8
00:00:28,920 --> 00:00:34,210
והנה מודל הרגרסיה הליניארית עם פונקציית ההשערה הליניארית

9
00:00:34,210 --> 00:00:36,540
ופונקצית העלות של ריבוע השגיאות.

10
00:00:36,540 --> 00:00:42,312
מה שאנחנו נעשה עכשיו הוא להחיל את הירידה במדרון

11
00:00:42,312 --> 00:00:47,820
כדי למזער את פונקצית העלות הזו.

12
00:00:47,820 --> 00:00:51,275
כדי להחיל את הירידה במדרון,

13
00:00:51,275 --> 00:00:59,810
לממש את קטע הקוד הזה, הביטוי המרכזי שאנחנו צריכים הוא הנגזרת הזו כאן.

14
00:00:59,810 --> 00:01:04,060
אז צריך להבין מה היא הנגזרת החלקית הזו,

15
00:01:04,060 --> 00:01:07,710
והצבה של ההגדרה של פונקצית העלות J

16
00:01:07,710 --> 00:01:11,670
נותנת את הדבר הזה.

17
00:01:13,020 --> 00:01:15,550
סכום מ-i שווה 1 עד m.

18
00:01:15,550 --> 00:01:21,400
של ריבוע השגיאה של האיבר ה-i בפונקצית העלות.

19
00:01:21,400 --> 00:01:23,520
וכל מה שעשיתי כאן היה פשוט

20
00:01:23,520 --> 00:01:26,190
להציב כאן את ההגדרה של פונקצית העלות.

21
00:01:27,290 --> 00:01:34,820
וכשנפשט את זה עוד קצת, זה יצא שווה לזה.

22
00:01:34,820 --> 00:01:43,280
הסכום מאחד עד m של θ₀ פלוס θ₁ כפול xi מינוס yi בריבוע.

23
00:01:43,280 --> 00:01:47,830
וכל מה שעשיתי היה לקחת את הגדרת פונקצית ההשערה

24
00:01:47,830 --> 00:01:50,782
שלי ולהציב את זה שם.

25
00:01:50,782 --> 00:01:53,190
אז אנחנו צריכים לחשב מה היא הנגזרת החלקית הזו

26
00:01:53,190 --> 00:01:56,570
עבור שני מקרים, j=0 ו-j=1.

27
00:01:56,570 --> 00:02:00,310
אנחנו רוצים לדעת את הנגזרת החלקית

28
00:02:00,310 --> 00:02:04,170
ל-θ₀ ו-θ₁.

29
00:02:04,170 --> 00:02:06,940
ואני פשוט אכתוב את התשובות.

30
00:02:06,940 --> 00:02:12,064
מתברר שהביטוי הראשון הוא 1 חלקי m

31
00:02:12,064 --> 00:02:18,354
כפול הסכום של השגיאות של כל סדרת האימון, (h(x(i))-y(i

32
00:02:18,354 --> 00:02:24,294
ועבור הנגזרת החלקית הזו של θ₁,

33
00:02:24,294 --> 00:02:27,114
נקבל את הביטוי הזה.

34
00:02:27,114 --> 00:02:34,008
(h(x(i))-y(i כפול (x(i.

35
00:02:34,008 --> 00:02:37,440
כשמחשבים

36
00:02:37,440 --> 00:02:41,720
את הנגזרות החלקיות האלה, אנחנו הולכים מהמשוואה הזו

37
00:02:41,720 --> 00:02:46,000
אל כל אחת מהמשוואות האלה.

38
00:02:46,000 --> 00:02:51,020
החישוב של הנגזרות החלקיות האלה דורש קצת חשבון דיפרנציאלי.

39
00:02:51,020 --> 00:02:54,930
אם אתה יודע חשבון דיפרנציאלי, תרגיש חופשי לנסות בעצמך

40
00:02:54,930 --> 00:02:59,510
ולראות שאם אתה מחשב, אתה מקבל אותן התשובות שאני קיבלתי.

41
00:02:59,510 --> 00:03:04,050
אבל אם אתה פחות מכיר חשבון דיפרנציאלי, אל תדאג

42
00:03:04,050 --> 00:03:08,100
וזה בסדר פשוט להשתמש במשוואות האלה שיצאו לנו,

43
00:03:08,100 --> 00:03:11,350
לא חייבים לדעת חשבון דיפרנציאלי כדי לעשות את שיעורי הבית

44
00:03:11,350 --> 00:03:13,390
אז בואו פשוט ניישם את הירידה במדרון ונחזור לעבודה.

45
00:03:14,750 --> 00:03:18,490
אז חמושים בהגדרות אלה ובנגזרות

46
00:03:18,490 --> 00:03:22,310
שחישבנו, שהם בעצם פשוט השיפוע של פונקצית העלות J

47
00:03:23,310 --> 00:03:27,160
עכשיו אנחנו יכולים להציב אותם בחזרה לתוך אלגוריתם הירידה במדרון שלנו.

48
00:03:27,160 --> 00:03:28,640
אז הנה הירידה במדרון

49
00:03:28,640 --> 00:03:32,728
עבור רגרסיה ליניארית, אותה מריצים חזור והרץ עד להתכנסות, מפחיתים מ-θ₀

50
00:03:32,728 --> 00:03:38,380
ו-θ₁ פחות α כפול הנגזרת הזו,

51
00:03:39,390 --> 00:03:41,070
הביטוי הזה כאן.

52
00:03:43,080 --> 00:03:46,050
אז הנה אלגוריתם הרגרסיה הליניארית שלנו.

53
00:03:47,160 --> 00:03:48,628
הביטוי הראשון הזה שכאן.

54
00:03:52,529 --> 00:03:56,804
הביטוי הזה הוא כמובן פשוט הנגזרת החלקית לפי θ₀,

55
00:03:56,804 --> 00:03:59,790
שחישבנו בשקופית קודמת.

56
00:03:59,790 --> 00:04:05,730
וגם הביטוי השני הזה כאן, הביטוי הזה הוא פשוט הנגזרת החלקית

57
00:04:05,730 --> 00:04:11,420
לפי θ₁, שגם אותה חישבנו בשקופית הקודמת.

58
00:04:11,420 --> 00:04:15,230
ולתזכורת, כשמיישמים ירידה במדרון חייבים

59
00:04:15,230 --> 00:04:19,265
לזכור את הפרט הזה שצריך לבנות את היישום

60
00:04:19,265 --> 00:04:22,250
כך שיעדכן את θ₀ ו-θ₁ בו-זמנית.

61
00:04:24,290 --> 00:04:25,570
אז

62
00:04:25,570 --> 00:04:28,120
בואו נראה איך עובדת הירידה במדרון.

63
00:04:28,120 --> 00:04:31,862
אחת הסוגיות שראינו עם הירידה במדרון היא שהוא עשוי להיות רגיש

64
00:04:31,862 --> 00:04:32,700
לאופטימום מקומי.

65
00:04:32,700 --> 00:04:36,780
כשהסברתי את הירידה במדרון בהתחלה הראיתי את התמונה הזאת

66
00:04:36,780 --> 00:04:40,900
של איך הוא מתגלגל בירידה על פני השטח, וראינו שלפי המיקום של הנקודה הראשונית,

67
00:04:40,900 --> 00:04:43,014
אפשר להגיע בסוף למינימום מקומי שונה.

68
00:04:43,014 --> 00:04:45,480
נגיע או לכאן או לכאן.

69
00:04:45,480 --> 00:04:50,390
אבל מתברר שפונקציית העלות

70
00:04:50,390 --> 00:04:55,220
עבור רגרסיה ליניארית היא תמיד פונקציה כזו בצורת קערה.

71
00:04:55,220 --> 00:05:00,190
המונח הטכני לכך הוא שזוהי פונקציה קמורה.

72
00:05:03,230 --> 00:05:07,800
ואני לא נותן כאן הגדרה פורמלית של מה זו פונקציה קמורה,

73
00:05:07,800 --> 00:05:09,490
C, O, N, V, E, X.

74
00:05:09,490 --> 00:05:16,620
אבל באופן לא פורמלי, פונקציה קמורה פרושה פונקציה בצורת קערה

75
00:05:16,620 --> 00:05:22,295
ולפונקציה כזו אין שום נקודות אופטימום מקומיות למעט האופטימום הגלובלי האחד.

76
00:05:22,295 --> 00:05:26,465
ולכן הירידה במדרון על סוג כזה של פונקציית עלות

77
00:05:26,465 --> 00:05:30,445
שהיא הדבר היחיד שיכול להיות ברגרסיה ליניארית תמיד תתכנס לאופטימום הגלובלי.

78
00:05:30,445 --> 00:05:33,155
כי אין לה שום אופטימום אחר, גלובלי או מקומי.

79
00:05:33,155 --> 00:05:36,615
אז עכשיו בואו נראה את האלגוריתם הזה בפעולה.

80
00:05:38,250 --> 00:05:45,910
כרגיל, כאן ציירתי את פונקצית ההשערה ואת פונקצית העלות J.

81
00:05:45,910 --> 00:05:50,020
ונניח שאתחלתי את הפרמטרים שלי בערכים האלה.

82
00:05:50,020 --> 00:05:54,220
בדרך כלל מאתחלים את הפרמטרים באפס אפס.

83
00:05:54,220 --> 00:05:56,370
θ₀ ו-θ₁ שניהם שווים אפס.

84
00:05:56,370 --> 00:06:01,354
אבל לשם ההדגמה

85
00:06:01,354 --> 00:06:07,619
אתחלתי אותם כך, θ₀ שווה 900 ו-θ₁ בסביבות 0.1-, בסדר.

86
00:06:07,619 --> 00:06:12,644
הפונקציה מתאימה לקו h(x)=-900-0.1x,

87
00:06:12,644 --> 00:06:16,547
הקו הזה כאן על פונקציית העלות.

88
00:06:16,547 --> 00:06:21,060
עכשיו, אם נעשה צעד אחד של הירידה במדרון,

89
00:06:21,060 --> 00:06:26,845
נגיע מהנקודה הזו כאן, כלפי למטה

90
00:06:26,845 --> 00:06:31,510
ושמאלה, לנקודה השנייה הזו שם.

91
00:06:31,510 --> 00:06:35,450
ואפשר לשים לב שהקו משתנה קצת,

92
00:06:35,450 --> 00:06:39,780
וכשעושים עוד צעד של הירידה במדרון, הקו מהשמאל שוב משתנה.

93
00:06:41,230 --> 00:06:42,380
נכון?

94
00:06:42,380 --> 00:06:46,370
וגם עברנו לנקודה חדשה על פונקצית העלות.

95
00:06:47,670 --> 00:06:52,760
וכשנמשיך לעשות צעדים בירידה במדרון, גם המחיר יורד.

96
00:06:52,760 --> 00:06:56,190
אז הפרמטרים עוברים דרך המסלול הזה.

97
00:06:57,340 --> 00:07:02,430
ואם תסתכלו על צד שמאל, גם פונקצית ההשערב עוברת אותו תהליך.

98
00:07:02,430 --> 00:07:06,520
ההתאמה שלה לנתונים הולכת ומשתפרת

99
00:07:08,200 --> 00:07:14,660
עד שלבסוף אנחנו מגיעים למינימום הגלובלי, המינימום הגלובלי הזה

100
00:07:14,660 --> 00:07:20,090
מתאים לפונקצית ההשערה הזו, שהיא מתאימה מאוד לנתונים.

101
00:07:21,400 --> 00:07:25,800
אז זהו הירידה במדרון, הפעלנו אותו

102
00:07:25,800 --> 00:07:31,230
וקבלנו התאמה טובה לסדרת הנתונים של מחירי הבתים.

103
00:07:31,230 --> 00:07:34,490
ועכשיו אפשר להשתמש בו כדי לחזות

104
00:07:34,490 --> 00:07:38,900
אם לחבר שלך יש בית בגודל 1250 רגל מרובעת,

105
00:07:38,900 --> 00:07:43,350
עכשיו אתה יכול לקרוא בגרף את הערך ולהגיד להם שהם יכולים לקבל

106
00:07:43,350 --> 00:07:48,720
אולי $250,000 עבור הבית שלהם.

107
00:07:48,720 --> 00:07:52,620
יש לאלגוריתם הזה גם שם אחר, מתברר שהאלגוריתם

108
00:07:52,620 --> 00:07:57,510
עליו עברנו כרגע נקרא לפעמים הירידה במדרון או בשיפוע באצְוָוה.

109
00:07:57,510 --> 00:08:00,730
מסתבר שבלמידה חישובית, אני מרגיש שאנשי למידה חישובית

110
00:08:00,730 --> 00:08:04,310
כמונו לא תמיד הכי מוצלחים במתן שמות לאלגוריתמים.

111
00:08:04,310 --> 00:08:08,880
אבל המונח הירידה בשיפוע באצְוָוה מתייחס לעובדה

112
00:08:08,880 --> 00:08:13,850
שבכל צעד של הירידה בשיפוע אנחנו מתחשבים בכל דוגמאות האימון.

113
00:08:13,850 --> 00:08:17,760
אז בירידה בשיפוע, בחישוב הנגזרת,

114
00:08:17,760 --> 00:08:21,400
אנחנו מחשבים את הסכומים האלה.

115
00:08:21,400 --> 00:08:25,660
אז בכל צעד של ירידה בשיפוע אנחנו מחשבים משהו כזה שמסכם

116
00:08:25,660 --> 00:08:30,620
את כל דוגמאות האימון שלנו ולכן המונח ירידה בשיפוע באצווה מתייחס

117
00:08:30,620 --> 00:08:34,175
לעובדה שאנחנו מסתכלים על כל האצווה של דוגמאות אימון.

118
00:08:34,175 --> 00:08:36,365
ושוב, זה ממש לא שם טוב, אבל

119
00:08:36,365 --> 00:08:39,585
כך קוראים לזה אנשי למידת מכונה.

120
00:08:39,585 --> 00:08:43,715
ומתברר שיש לפעמים גרסאות אחרות של ירידה במדרון

121
00:08:43,715 --> 00:08:46,247
שאינם גרסאות אצווה, אלא במקום זה

122
00:08:46,247 --> 00:08:48,837
לא מסתכלים על כל סדרת האימון אלא

123
00:08:48,837 --> 00:08:51,247
על תת סדרה קטנה של דוגמאות האימון בכל פעם.

124
00:08:51,247 --> 00:08:55,207
ונדבר גם על הגירסאות האלה בהמשך הקורס.

125
00:08:55,207 --> 00:08:58,357
אבל לעת עתה באמצעות האלגוריתם שלמדנו, ירידה בשיפוע באצְוָוה

126
00:08:58,357 --> 00:09:03,497
עכשיו אנחנו יודעים כיצד ליישם ירידה בשיפוע עבור רגרסיה ליניארית.

127
00:09:05,980 --> 00:09:09,550
אז זהו זה בקשר לרגרסיה ליניארית עם ירידה במדרון.

128
00:09:09,550 --> 00:09:12,260
אם כבר למדת אלגברה לינארית מתקדמת,

129
00:09:12,260 --> 00:09:15,510
חלק מכם אולי כבר עבר קורס של אלגברה לינארית מתקדמת.

130
00:09:15,510 --> 00:09:19,410
אז אתה אולי יודע שקיימת נוסחה לפתרון

131
00:09:19,410 --> 00:09:22,270
של המינימום של פונקצית העלות J ללא צורך

132
00:09:22,270 --> 00:09:25,870
בשימוש באלגוריתם איטרטיבי כמו ירידה בשיפוע.

133
00:09:25,870 --> 00:09:29,730
בהמשך הקורס נדבר גם על השיטה שמוצאת

134
00:09:29,730 --> 00:09:33,020
את המינימום של פונקציה העלות J ללא צורך בשלבים המרובים האלה

135
00:09:33,020 --> 00:09:34,520
של הירידה בשיפוע.

136
00:09:34,520 --> 00:09:37,020
השיטה האחרת הזו נקראת שיטת המשוואות הנורמליות.

137
00:09:37,020 --> 00:09:41,000
אבל אם במקרה שמעת על השיטה, מתברר כי שלירידה בשיפוע

138
00:09:41,000 --> 00:09:46,420
יש ביצועים עדיפים כאשר מדובר בערכות נתונים גדולות יותר מאשר שיטת המשוואות הנורמליות.

139
00:09:46,420 --> 00:09:50,140
ועכשיו שאנחנו יודעים על ירידה בשיפוע נוכל להשתמש בו בהקשרים

140
00:09:50,140 --> 00:09:51,400
רבים ושונים

141
00:09:51,400 --> 00:09:53,910
ואנחנו נשתמש בו גם בהרבה מאוד בעיות שונות של למידת מכונה.

142
00:09:55,340 --> 00:10:00,430
אז איחוליי על למידתכם על האלגוריתם הראשון שלכם בלמידה חישובית.

143
00:10:00,430 --> 00:10:04,990
מאוחר יותר נעשה תרגילים שבהם נבקש מכם ליישם את הירידה בשיפוע

144
00:10:04,990 --> 00:10:07,480
ובתקווה שתשתמשו באלגוריתמים האלה ממש בעצמכם.

145
00:10:07,480 --> 00:10:11,460
אבל קודם כל אני רוצה לספר לכם על הסדרה הבאה של קטעי וידאו.

146
00:10:11,460 --> 00:10:14,510
ובראש ובראשונה נספר לכם על הכללה של

147
00:10:14,510 --> 00:10:17,900
אלגוריתם הירידה במדרון שתעשה אותו הרבה יותר חזק.

148
00:10:17,900 --> 00:10:20,420
ואני מניח שאני אספר לכם על זה בסרטון הבא.