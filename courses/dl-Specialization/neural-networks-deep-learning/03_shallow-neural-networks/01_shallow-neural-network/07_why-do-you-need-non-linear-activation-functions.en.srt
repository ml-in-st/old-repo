1
00:00:00,030 --> 00:00:04,710
why does your new network need a

2
00:00:02,220 --> 00:00:06,089
nonlinear activation function turns out

3
00:00:04,710 --> 00:00:08,160
that for your new network to compute

4
00:00:06,089 --> 00:00:09,990
interesting functions you do need to

5
00:00:08,160 --> 00:00:12,990
take a nonlinear activation function

6
00:00:09,990 --> 00:00:15,990
less you want so just the for prop

7
00:00:12,990 --> 00:00:18,029
equations for the neural network why

8
00:00:15,990 --> 00:00:22,160
don't we just get rid of this get rid of

9
00:00:18,029 --> 00:00:25,920
the function G and set a1 equals Z 1 or

10
00:00:22,160 --> 00:00:28,289
alternatively you could say that G of Z

11
00:00:25,920 --> 00:00:31,470
is equal to Z right sometimes this is

12
00:00:28,289 --> 00:00:33,210
called the linear activation function

13
00:00:31,470 --> 00:00:35,280
maybe a better name for it would be the

14
00:00:33,210 --> 00:00:37,280
identity activation function because

15
00:00:35,280 --> 00:00:39,270
they're just outputs whatever was input

16
00:00:37,280 --> 00:00:44,010
for the purpose of this

17
00:00:39,270 --> 00:00:46,410
what if a2 was just equal to z2 it turns

18
00:00:44,010 --> 00:00:50,730
out if you do this then this model is

19
00:00:46,410 --> 00:00:54,390
just computing Y or Y hat as a linear

20
00:00:50,730 --> 00:00:57,170
function of your input features x2 take

21
00:00:54,390 --> 00:01:05,970
the first two equations if you have that

22
00:00:57,170 --> 00:01:12,720
a1 is equal to z1 is equal to w1 X plus

23
00:01:05,970 --> 00:01:21,869
B and if then a2 is equal to z2 is equal

24
00:01:12,720 --> 00:01:24,900
to W 2 a1 plus B then if you take the

25
00:01:21,869 --> 00:01:32,009
definition of a1 and plug it in there

26
00:01:24,900 --> 00:01:38,520
you find that a2 is equal to W 2 times W

27
00:01:32,009 --> 00:01:43,920
1 X plus b1 a bit all right so this is

28
00:01:38,520 --> 00:01:59,120
um a 1 plus B 2 and so this simplifies

29
00:01:43,920 --> 00:02:06,120
to W 2 W 1 X plus W 2 b1 plus b2 so this

30
00:01:59,120 --> 00:02:08,930
it's just let's call this w prime b

31
00:02:06,120 --> 00:02:10,710
prime so it is just equal to w prime X

32
00:02:08,930 --> 00:02:12,720
plus B Prime

33
00:02:10,710 --> 00:02:15,690
if you were to use linear activation

34
00:02:12,720 --> 00:02:18,690
functions or we go to call them identity

35
00:02:15,690 --> 00:02:20,940
activation functions then the new

36
00:02:18,690 --> 00:02:24,390
network is just outputting a linear

37
00:02:20,940 --> 00:02:26,940
function of the input and we'll talk

38
00:02:24,390 --> 00:02:28,740
about deep networks later new networks

39
00:02:26,940 --> 00:02:31,260
with many many layers many many hidden

40
00:02:28,740 --> 00:02:33,570
layers and it turns out that if you use

41
00:02:31,260 --> 00:02:35,070
a linear activation function or

42
00:02:33,570 --> 00:02:37,140
alternatively if you don't have an

43
00:02:35,070 --> 00:02:38,790
activation function then no matter how

44
00:02:37,140 --> 00:02:41,610
many layers your neural network has

45
00:02:38,790 --> 00:02:43,590
always doing is just computing a linear

46
00:02:41,610 --> 00:02:46,980
activation function so you might as well

47
00:02:43,590 --> 00:02:49,770
not have any hidden layers some of the

48
00:02:46,980 --> 00:02:52,020
cases that briefly mentioned it turns

49
00:02:49,770 --> 00:02:54,510
out that if you have a linear activation

50
00:02:52,020 --> 00:02:57,000
function here and a sigmoid function

51
00:02:54,510 --> 00:02:59,010
here then this model is no more

52
00:02:57,000 --> 00:03:02,700
expressive than standard logistic

53
00:02:59,010 --> 00:03:04,440
regression without any hidden layer so I

54
00:03:02,700 --> 00:03:06,360
won't bother to prove that but you could

55
00:03:04,440 --> 00:03:09,240
try to do so if you want but the

56
00:03:06,360 --> 00:03:12,690
take-home is that a linear hidden layer

57
00:03:09,240 --> 00:03:15,000
is more or less useless because on the

58
00:03:12,690 --> 00:03:17,850
composition of two linear functions is

59
00:03:15,000 --> 00:03:20,190
itself a linear function so unless you

60
00:03:17,850 --> 00:03:21,989
throw a non-linearity in there then

61
00:03:20,190 --> 00:03:24,060
you're not computing more interesting

62
00:03:21,989 --> 00:03:27,209
functions even as you go deeper in the

63
00:03:24,060 --> 00:03:28,739
network there is just one place where

64
00:03:27,209 --> 00:03:32,820
you might use a linear activation

65
00:03:28,739 --> 00:03:35,640
function G of Z equals Z and that's if

66
00:03:32,820 --> 00:03:38,790
you are doing machine learning on a

67
00:03:35,640 --> 00:03:41,130
regression problem so if Y is a real

68
00:03:38,790 --> 00:03:44,640
number so for example if you're trying

69
00:03:41,130 --> 00:03:47,010
to predict housing prices so Y is a it's

70
00:03:44,640 --> 00:03:50,430
not 0 1 but it's a real number you know

71
00:03:47,010 --> 00:03:53,340
anywhere from zero dollars is a price of

72
00:03:50,430 --> 00:03:55,650
holes up to however expensive right

73
00:03:53,340 --> 00:03:58,530
house of kin I guess maybe however can

74
00:03:55,650 --> 00:04:03,000
be you know potentially millions of

75
00:03:58,530 --> 00:04:07,160
dollars so however however much houses

76
00:04:03,000 --> 00:04:11,670
cost in your data set but if Y takes on

77
00:04:07,160 --> 00:04:13,829
these real values then it might be OK to

78
00:04:11,670 --> 00:04:19,380
have a linear activation function here

79
00:04:13,829 --> 00:04:21,060
so that your output Y hat is also a real

80
00:04:19,380 --> 00:04:24,750
number going anywhere from minus

81
00:04:21,060 --> 00:04:27,630
infinity to plus infinity but then the

82
00:04:24,750 --> 00:04:29,580
hidden units should not use the new

83
00:04:27,630 --> 00:04:33,180
activation functions they could use relu

84
00:04:29,580 --> 00:04:35,370
or 10h or these you relu or maybe

85
00:04:33,180 --> 00:04:37,020
something else so the one place you

86
00:04:35,370 --> 00:04:40,230
might use a linear activation function

87
00:04:37,020 --> 00:04:43,350
others usually in the output layer but

88
00:04:40,230 --> 00:04:46,820
other than that using a linear

89
00:04:43,350 --> 00:04:48,690
activation function in a fitting layer

90
00:04:46,820 --> 00:04:51,540
except for some very special

91
00:04:48,690 --> 00:04:53,640
circumstances relating to compression

92
00:04:51,540 --> 00:04:55,200
that won't want to talk about using a

93
00:04:53,640 --> 00:04:57,180
linear activation function is extremely

94
00:04:55,200 --> 00:04:59,100
rare oh and of course today actually

95
00:04:57,180 --> 00:05:01,230
predicting housing prices as you saw on

96
00:04:59,100 --> 00:05:03,450
the week 1 video because housing prices

97
00:05:01,230 --> 00:05:06,210
are all non-negative perhaps even then

98
00:05:03,450 --> 00:05:09,180
you can use a value activation function

99
00:05:06,210 --> 00:05:12,030
so that your outputs Y hat are all

100
00:05:09,180 --> 00:05:14,040
greater than or equal to 0 so I hope

101
00:05:12,030 --> 00:05:16,170
that gives you a sense of why having a

102
00:05:14,040 --> 00:05:19,620
nonlinear activation function is a

103
00:05:16,170 --> 00:05:21,030
critical part of neural networks next

104
00:05:19,620 --> 00:05:24,330
we're going to start to talk about

105
00:05:21,030 --> 00:05:25,950
gradient descent and to do that to set

106
00:05:24,330 --> 00:05:28,320
up for discussion for gradient descent

107
00:05:25,950 --> 00:05:30,810
in the next video I want to show you how

108
00:05:28,320 --> 00:05:33,150
to estimate how to compute the slope of

109
00:05:30,810 --> 00:05:35,070
the derivative of individual activation

110
00:05:33,150 --> 00:05:37,430
functions so let's go on to the next

111
00:05:35,070 --> 00:05:37,430
video