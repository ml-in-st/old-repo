1
00:00:00,000 --> 00:00:02,825
In the last video, you learned about the GRU,

2
00:00:02,825 --> 00:00:04,170
the gated recurrent units,

3
00:00:04,170 --> 00:00:09,770
and how that can allow you to learn very long range connections in a sequence.

4
00:00:09,770 --> 00:00:12,765
The other type of unit that allows you to do this very well

5
00:00:12,765 --> 00:00:16,028
is the LSTM or the long short term memory units,

6
00:00:16,028 --> 00:00:20,110
and this is even more powerful than the GRU. Let's take a look.

7
00:00:20,110 --> 00:00:23,161
Here are the equations from the previous video for the GRU.

8
00:00:23,161 --> 00:00:24,354
And for the GRU,

9
00:00:24,354 --> 00:00:27,210
we had a_t equals c_t,

10
00:00:27,210 --> 00:00:31,615
and two gates, the optic gate and the relevance gate, c_tilde_t,

11
00:00:31,615 --> 00:00:34,946
which is a candidate for replacing the memory cell,

12
00:00:34,946 --> 00:00:38,267
and then we use the update gate, gamma_u,

13
00:00:38,267 --> 00:00:43,408
to decide whether or not to update c_t using c_tilde_t.

14
00:00:43,408 --> 00:00:49,985
The LSTM is an even slightly more powerful and more general version of the GRU,

15
00:00:49,985 --> 00:00:55,360
and is due to Sepp Hochreiter and Jurgen Schmidhuber.

16
00:00:55,360 --> 00:00:57,415
And this was a really seminal paper,

17
00:00:57,415 --> 00:01:01,555
a huge impact on sequence modelling.

18
00:01:01,555 --> 00:01:04,740
I think this paper is one of the more difficult to read.

19
00:01:04,740 --> 00:01:08,280
It goes quite along into theory of vanishing gradients.

20
00:01:08,280 --> 00:01:12,555
And so, I think more people have learned about the details of LSTM through

21
00:01:12,555 --> 00:01:15,510
maybe other places than from this particular paper even though I think

22
00:01:15,510 --> 00:01:19,345
this paper has had a wonderful impact on the Deep Learning community.

23
00:01:19,345 --> 00:01:23,176
But these are the equations that govern the LSTM.

24
00:01:23,176 --> 00:01:26,775
So, the book continued to the memory cell, c,

25
00:01:26,775 --> 00:01:30,480
and the candidate value for updating it, c_tilde_t,

26
00:01:30,480 --> 00:01:34,810
will be this, and so on.

27
00:01:34,810 --> 00:01:38,930
Notice that for the LSTM,

28
00:01:38,930 --> 00:01:45,800
we will no longer have the case that a_t is equal to c_t.

29
00:01:45,800 --> 00:01:48,000
So, this is what we use.

30
00:01:48,000 --> 00:01:51,830
And so, this is just like the equation on the left except that with now,

31
00:01:51,830 --> 00:01:56,095
more specially use a_t there or a_t minus one instead of c_t minus one.

32
00:01:56,095 --> 00:01:59,710
And we're not using this gamma or this relevance gate.

33
00:01:59,710 --> 00:02:03,615
Although you could have a variation of the LSTM where you put that back in,

34
00:02:03,615 --> 00:02:06,110
but with the more common version of the LSTM,

35
00:02:06,110 --> 00:02:07,900
doesn't bother with that.

36
00:02:07,900 --> 00:02:12,320
And then we will have an update gate, same as before.

37
00:02:12,320 --> 00:02:21,110
So, W updates and we're going to use a_t minus one here, x_t plus b_u.

38
00:02:21,110 --> 00:02:25,340
And one new property of the LSTM is,

39
00:02:25,340 --> 00:02:28,725
instead of having one update gate control,

40
00:02:28,725 --> 00:02:29,970
both of these terms,

41
00:02:29,970 --> 00:02:32,315
we're going to have two separate terms.

42
00:02:32,315 --> 00:02:35,795
So instead of gamma_u and one minus gamma_u,

43
00:02:35,795 --> 00:02:37,415
we're going have gamma_u here.

44
00:02:37,415 --> 00:02:41,910
And forget gate, which we're going to call gamma_f.

45
00:02:41,910 --> 00:02:43,790
So, this gate, gamma_f,

46
00:02:43,790 --> 00:02:48,695
is going to be sigmoid of pretty much what you'd

47
00:02:48,695 --> 00:02:54,866
expect, x_t plus b_f.

48
00:02:54,866 --> 00:03:01,180
And then, we're going to have a new output gate which is sigma of W_o.

49
00:03:01,180 --> 00:03:09,239
And then again, pretty much what you'd expect, plus b_o.

50
00:03:09,239 --> 00:03:17,560
And then, the update value to the memory so will be c_t equals gamma u.

51
00:03:17,560 --> 00:03:21,785
And this asterisk denotes element-wise multiplication.

52
00:03:21,785 --> 00:03:24,575
This is a vector-vector element-wise multiplication,

53
00:03:24,575 --> 00:03:27,525
plus, and instead of one minus gamma u,

54
00:03:27,525 --> 00:03:30,470
we're going to have a separate forget gate, gamma_f,

55
00:03:30,470 --> 00:03:34,475
times c of t minus one.

56
00:03:34,475 --> 00:03:37,970
So this gives the memory cell the option of keeping

57
00:03:37,970 --> 00:03:41,720
the old value c_t minus one and then just adding to it,

58
00:03:41,720 --> 00:03:43,160
this new value, c tilde of t. So,

59
00:03:43,160 --> 00:03:48,445
use a separate update and forget gates.

60
00:03:48,445 --> 00:03:53,960
So, this stands for update, forget, and output gate.

61
00:03:53,960 --> 00:04:02,413
And then finally, instead of a_t equals c_t a_t is a_t

62
00:04:02,413 --> 00:04:10,700
equal to the output gate element-wise multiplied by c_t.

63
00:04:10,700 --> 00:04:13,170
So, these are the equations that govern the LSTM

64
00:04:13,170 --> 00:04:18,070
and you can tell it has three gates instead of two.

65
00:04:18,070 --> 00:04:23,025
So, it's a bit more complicated and it places the gates into slightly different places.

66
00:04:23,025 --> 00:04:29,805
So, here again are the equations governing the behavior of the LSTM.

67
00:04:29,805 --> 00:04:33,450
Once again, it's traditional to explain these things using pictures.

68
00:04:33,450 --> 00:04:35,505
So let me draw one here.

69
00:04:35,505 --> 00:04:38,995
And if these pictures are too complicated, don't worry about it.

70
00:04:38,995 --> 00:04:42,940
I personally find the equations easier to understand than the picture.

71
00:04:42,940 --> 00:04:46,680
But I'll just show the picture here for the intuitions it conveys.

72
00:04:46,680 --> 00:04:52,210
The bigger picture here was very much inspired by a blog post due to Chris Ola, title,

73
00:04:52,210 --> 00:04:54,580
Understanding LSTM Network, and the diagram drawing

74
00:04:54,580 --> 00:04:58,540
here is quite similar to one that he drew in his blog post.

75
00:04:58,540 --> 00:05:02,460
But the key thing is to take away from this picture are maybe that you use

76
00:05:02,460 --> 00:05:06,900
a_t minus one and x_t to compute all the gate values.

77
00:05:06,900 --> 00:05:08,970
In this picture, you have a_t minus one,

78
00:05:08,970 --> 00:05:11,940
x_t coming together to compute the forget gate,

79
00:05:11,940 --> 00:05:13,665
to compute the update gates,

80
00:05:13,665 --> 00:05:16,140
and to compute output gate.

81
00:05:16,140 --> 00:05:21,765
And they also go through a tanh to compute c_tilde_of_t.

82
00:05:21,765 --> 00:05:23,790
And then these values are combined in

83
00:05:23,790 --> 00:05:27,384
these complicated ways with element-wise multiplies and so on,

84
00:05:27,384 --> 00:05:33,000
to get c_t from the previous c_t minus one.

85
00:05:33,000 --> 00:05:37,020
Now, one element of this is interesting as you have a bunch of these in parallel.

86
00:05:37,020 --> 00:05:39,605
So, that's one of them and you connect them.

87
00:05:39,605 --> 00:05:41,633
You then connect these temporally.

88
00:05:41,633 --> 00:05:45,545
So it does the input x_1 then x_2, x_3.

89
00:05:45,545 --> 00:05:51,020
So, you can take these units and just hold them up as follows,

90
00:05:51,020 --> 00:05:56,860
where the output a at the previous timestep is the input a at the next timestep,

91
00:05:56,860 --> 00:06:00,180
the c. I've simplified to diagrams a little bit in the bottom.

92
00:06:00,180 --> 00:06:03,350
And one cool thing about this you'll notice is that

93
00:06:03,350 --> 00:06:07,185
there's this line at the top that shows how,

94
00:06:07,185 --> 00:06:12,180
so long as you set the forget and the update gate appropriately,

95
00:06:12,180 --> 00:06:15,450
it is relatively easy for the LSTM to have

96
00:06:15,450 --> 00:06:21,380
some value c_0 and have that be passed all the way to the right to have your,

97
00:06:21,380 --> 00:06:23,434
maybe, c_3 equals c_0.

98
00:06:23,434 --> 00:06:25,095
And this is why the LSTM,

99
00:06:25,095 --> 00:06:26,895
as well as the GRU,

100
00:06:26,895 --> 00:06:31,585
is very good at memorizing certain values even for a long time,

101
00:06:31,585 --> 00:06:39,975
for certain real values stored in the memory cell even for many, many timesteps.

102
00:06:39,975 --> 00:06:41,970
So, that's it for the LSTM.

103
00:06:41,970 --> 00:06:44,965
As you can imagine,

104
00:06:44,965 --> 00:06:48,690
there are also a few variations on this that people use.

105
00:06:48,690 --> 00:06:52,410
Perhaps, the most common one is that instead of just having

106
00:06:52,410 --> 00:06:57,094
the gate values be dependent only on a_t minus one, x_t,

107
00:06:57,094 --> 00:07:05,745
sometimes, people also sneak in there the values c_t minus one as well.

108
00:07:05,745 --> 00:07:10,311
This is called a peephole connection.

109
00:07:10,311 --> 00:07:13,595
Not a great name maybe but you'll see, peephole connection.

110
00:07:13,595 --> 00:07:19,975
What that means is that the gate values may depend not just on a_t minus one and on x_t,

111
00:07:19,975 --> 00:07:23,090
but also on the previous memory cell value,

112
00:07:23,090 --> 00:07:28,615
and the peephole connection can go into all three of these gates' computations.

113
00:07:28,615 --> 00:07:32,985
So that's one common variation you see of LSTMs.

114
00:07:32,985 --> 00:07:38,200
One technical detail is that these are, say, 100-dimensional vectors.

115
00:07:38,200 --> 00:07:41,975
So if you have a 100-dimensional hidden memory cell unit, and so is this.

116
00:07:41,975 --> 00:07:46,450
And the, say, fifth element

117
00:07:46,450 --> 00:07:51,045
of c_t minus one affects only the fifth element of the corresponding gates,

118
00:07:51,045 --> 00:07:54,070
so that relationship is one-to-one,

119
00:07:54,070 --> 00:07:56,035
where not every element of

120
00:07:56,035 --> 00:07:59,850
the 100-dimensional c_t minus one can affect all elements of the case.

121
00:07:59,850 --> 00:08:04,720
But instead, the first element of c_t minus one affects the first element of the case,

122
00:08:04,720 --> 00:08:07,395
second element affects the second element, and so on.

123
00:08:07,395 --> 00:08:10,870
But if you ever read the paper and see someone talk about the peephole connection,

124
00:08:10,870 --> 00:08:16,630
that's when they mean that c_t minus one is used to affect the gate value as well.

125
00:08:16,630 --> 00:08:19,720
So, that's it for the LSTM.

126
00:08:19,720 --> 00:08:21,350
When should you use a GRU?

127
00:08:21,350 --> 00:08:23,285
And when should you use an LSTM?

128
00:08:23,285 --> 00:08:25,865
There isn't widespread consensus in this.

129
00:08:25,865 --> 00:08:28,755
And even though I presented GRUs first,

130
00:08:28,755 --> 00:08:30,730
in the history of deep learning,

131
00:08:30,730 --> 00:08:33,020
LSTMs actually came much earlier,

132
00:08:33,020 --> 00:08:37,092
and then GRUs were relatively recent invention that were maybe

133
00:08:37,092 --> 00:08:41,725
derived as Pavia's simplification of the more complicated LSTM model.

134
00:08:41,725 --> 00:08:44,860
Researchers have tried both of these models on many different problems,

135
00:08:44,860 --> 00:08:46,350
and on different problems,

136
00:08:46,350 --> 00:08:47,840
different algorithms will win out.

137
00:08:47,840 --> 00:08:51,070
So, there isn't a universally-superior algorithm

138
00:08:51,070 --> 00:08:53,630
which is why I want to show you both of them.

139
00:08:53,630 --> 00:08:56,970
But I feel like when I am using these,

140
00:08:56,970 --> 00:09:00,170
the advantage of the GRU is that it's a simpler model

141
00:09:00,170 --> 00:09:03,465
and so it is actually easier to build a much bigger network,

142
00:09:03,465 --> 00:09:04,780
it only has two gates,

143
00:09:04,780 --> 00:09:06,940
so computationally, it runs a bit faster.

144
00:09:06,940 --> 00:09:10,630
So, it scales the building somewhat bigger models but the LSTM

145
00:09:10,630 --> 00:09:15,465
is more powerful and more effective since it has three gates instead of two.

146
00:09:15,465 --> 00:09:17,875
If you want to pick one to use,

147
00:09:17,875 --> 00:09:21,550
I think LSTM has been the historically more proven choice.

148
00:09:21,550 --> 00:09:23,015
So, if you had to pick one,

149
00:09:23,015 --> 00:09:28,510
I think most people today will still use the LSTM as the default first thing to try.

150
00:09:28,510 --> 00:09:30,490
Although, I think in the last few years,

151
00:09:30,490 --> 00:09:35,050
GRUs had been gaining a lot of momentum and I feel like more and more teams

152
00:09:35,050 --> 00:09:39,835
are also using GRUs because they're a bit simpler but often work just as well.

153
00:09:39,835 --> 00:09:43,920
It might be easier to scale them to even bigger problems.

154
00:09:43,920 --> 00:09:46,065
So, that's it for LSTMs.

155
00:09:46,065 --> 00:09:48,607
Well, either GRUs or LSTMs,

156
00:09:48,607 --> 00:09:53,430
you'll be able to build neural network that can capture a much longer range depends.