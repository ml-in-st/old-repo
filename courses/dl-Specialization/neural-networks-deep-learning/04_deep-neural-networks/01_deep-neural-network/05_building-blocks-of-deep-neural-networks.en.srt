1
00:00:00,060 --> 00:00:04,830
in the earlier videos from this week as

2
00:00:02,399 --> 00:00:06,810
well as from the videos from the past

3
00:00:04,830 --> 00:00:08,490
several weeks you've already seen the

4
00:00:06,810 --> 00:00:10,710
basic building blocks of board

5
00:00:08,490 --> 00:00:12,929
propagation and back propagation the key

6
00:00:10,710 --> 00:00:15,089
components you need to implement a deep

7
00:00:12,929 --> 00:00:17,130
neural network let's see how you can put

8
00:00:15,089 --> 00:00:19,740
these components together to build a

9
00:00:17,130 --> 00:00:23,369
deep net use the network with a few

10
00:00:19,740 --> 00:00:26,250
layers let's pick one layer and look at

11
00:00:23,369 --> 00:00:29,699
the computations focusing on just that

12
00:00:26,250 --> 00:00:34,079
layer for now so for layer L you have

13
00:00:29,699 --> 00:00:39,230
some parameters WL and Bo and for the

14
00:00:34,079 --> 00:00:42,450
forward prop you will input the

15
00:00:39,230 --> 00:00:49,289
activations a L minus one from the

16
00:00:42,450 --> 00:00:52,710
previous layer and output Al so the way

17
00:00:49,289 --> 00:00:59,600
we did this previously was you compute Z

18
00:00:52,710 --> 00:01:08,280
l equals WL x al minus one plus BL um

19
00:00:59,600 --> 00:01:10,710
and then al equals G of Z L right so

20
00:01:08,280 --> 00:01:13,500
that's how you go from the input al

21
00:01:10,710 --> 00:01:16,320
minus one to the output Al and it turns

22
00:01:13,500 --> 00:01:21,390
out that for later use will be useful to

23
00:01:16,320 --> 00:01:23,909
also cache the value ZL so let me

24
00:01:21,390 --> 00:01:27,119
include this on cache as well because

25
00:01:23,909 --> 00:01:29,820
storing the value Z L will be useful for

26
00:01:27,119 --> 00:01:32,610
backward for the back propagation step

27
00:01:29,820 --> 00:01:34,200
later and then for the backward step or

28
00:01:32,610 --> 00:01:36,329
three for the back propagation step

29
00:01:34,200 --> 00:01:39,509
again focusing on computation for this

30
00:01:36,329 --> 00:01:46,939
layer L you're going to implement a

31
00:01:39,509 --> 00:01:53,130
function that inputs da of L and outputs

32
00:01:46,939 --> 00:01:55,680
da L minus one and just a special the

33
00:01:53,130 --> 00:02:00,450
details the input is actually da FL as

34
00:01:55,680 --> 00:02:03,420
well as the cache so you have available

35
00:02:00,450 --> 00:02:04,400
to you the value of ZL that you compute

36
00:02:03,420 --> 00:02:07,100
it and

37
00:02:04,400 --> 00:02:08,060
in addition to outputting GL minus 1 you

38
00:02:07,100 --> 00:02:10,700
will output

39
00:02:08,060 --> 00:02:12,830
you know the gradients you want in order

40
00:02:10,700 --> 00:02:15,730
to implement gradient descent for

41
00:02:12,830 --> 00:02:18,830
learning ok so this is the basic

42
00:02:15,730 --> 00:02:20,690
structure of how you implement this

43
00:02:18,830 --> 00:02:22,280
forward step I'm going to call it a

44
00:02:20,690 --> 00:02:24,160
forward function as well as backward

45
00:02:22,280 --> 00:02:28,160
step we shall call it back wave function

46
00:02:24,160 --> 00:02:30,530
so just to summarize in layer L you're

47
00:02:28,160 --> 00:02:31,730
going to have you know the forward step

48
00:02:30,530 --> 00:02:38,630
or the forward property' forward

49
00:02:31,730 --> 00:02:42,020
function input a L minus 1 and output al

50
00:02:38,630 --> 00:02:45,410
and in order to make this computation

51
00:02:42,020 --> 00:02:51,770
you need to use WL n PL um and also

52
00:02:45,410 --> 00:02:54,740
output a cache which contains ZL and

53
00:02:51,770 --> 00:02:59,470
then on the backward function using the

54
00:02:54,740 --> 00:03:06,770
back prop step will be another function

55
00:02:59,470 --> 00:03:11,269
then now inputs the AFL and outputs da

56
00:03:06,770 --> 00:03:13,400
ll minus 1 so it tells you given the

57
00:03:11,269 --> 00:03:16,970
derivatives respect to these activations

58
00:03:13,400 --> 00:03:19,760
that's da FL what are the derivatives or

59
00:03:16,970 --> 00:03:22,220
how much do I wish you know a L minus 1

60
00:03:19,760 --> 00:03:24,640
changes computed derivatives respect to

61
00:03:22,220 --> 00:03:27,860
D activations from the previous layer

62
00:03:24,640 --> 00:03:31,070
within this box right you need to use WL

63
00:03:27,860 --> 00:03:35,239
and BL and it turns out along the way

64
00:03:31,070 --> 00:03:38,209
you end up computing DZ l um and then

65
00:03:35,239 --> 00:03:44,660
this box this backward function can also

66
00:03:38,209 --> 00:03:46,790
output DW l and DB l well now sometimes

67
00:03:44,660 --> 00:03:48,610
using red arrows to denote the backward

68
00:03:46,790 --> 00:03:52,459
generations so if you prefer we could

69
00:03:48,610 --> 00:03:55,640
draw these arrows in red so if you can

70
00:03:52,459 --> 00:03:57,799
implement these two functions then the

71
00:03:55,640 --> 00:04:00,440
basic computation of the mirror network

72
00:03:57,799 --> 00:04:04,250
will be as follows you're going to take

73
00:04:00,440 --> 00:04:06,769
the input features a 0 see that in and

74
00:04:04,250 --> 00:04:10,340
that will compute the activations of the

75
00:04:06,769 --> 00:04:15,440
first layer let's call that a 1 and to

76
00:04:10,340 --> 00:04:17,840
do that you needed W 1 and B 1 and then

77
00:04:15,440 --> 00:04:22,790
we'll also you know cache

78
00:04:17,840 --> 00:04:25,639
the way z1 now having done that you feed

79
00:04:22,790 --> 00:04:28,820
that this is the second layer and then

80
00:04:25,639 --> 00:04:31,310
using W 2 and B 2 you're going to

81
00:04:28,820 --> 00:04:37,280
compute the activations our next layer a

82
00:04:31,310 --> 00:04:41,060
2 and so on until eventually you end up

83
00:04:37,280 --> 00:04:47,090
outputting a capital L which is equal to

84
00:04:41,060 --> 00:04:53,479
Y hat and along the way we cashed all of

85
00:04:47,090 --> 00:04:56,210
these on values Z so that's the forward

86
00:04:53,479 --> 00:04:58,790
propagation step now for the back

87
00:04:56,210 --> 00:05:03,169
propagation step what we're going to do

88
00:04:58,790 --> 00:05:06,620
will be a backward sequence of

89
00:05:03,169 --> 00:05:09,080
iterations in which you're going

90
00:05:06,620 --> 00:05:15,550
backwards and computing gradients like

91
00:05:09,080 --> 00:05:21,350
so so as you're going to feed in here da

92
00:05:15,550 --> 00:05:31,240
L and then this box will give us da of L

93
00:05:21,350 --> 00:05:33,770
minus 1 and so on until we get da - da 1

94
00:05:31,240 --> 00:05:37,550
you could actually get one more output

95
00:05:33,770 --> 00:05:39,650
to compute da 0 but this is derivative

96
00:05:37,550 --> 00:05:42,320
respect your input features which is not

97
00:05:39,650 --> 00:05:45,460
useful at least for training the weights

98
00:05:42,320 --> 00:05:48,680
of these are supervised neural networks

99
00:05:45,460 --> 00:05:50,479
so you could just stop it there belong

100
00:05:48,680 --> 00:05:56,180
the way back prop also ends up

101
00:05:50,479 --> 00:06:01,610
outputting DW l DB l right this used

102
00:05:56,180 --> 00:06:11,520
upon so wo and BL um this would output d

103
00:06:01,610 --> 00:06:15,440
w3 g p3 and so on so you enter

104
00:06:11,520 --> 00:06:15,440
computing all the derivatives you need

105
00:06:16,160 --> 00:06:22,259
until just a maybe so in the structure

106
00:06:19,770 --> 00:06:26,300
of this a little bit more right these

107
00:06:22,259 --> 00:06:26,300
boxes will use those parameters of slow

108
00:06:27,139 --> 00:06:34,740
WL PL and it turns out that we'll see

109
00:06:32,340 --> 00:06:37,949
later that inside these boxes we'll end

110
00:06:34,740 --> 00:06:39,720
up computing disease as well so one

111
00:06:37,949 --> 00:06:43,020
innovation of training for a new network

112
00:06:39,720 --> 00:06:46,729
involves starting with a zero which is X

113
00:06:43,020 --> 00:06:49,380
and going through for profit as follows

114
00:06:46,729 --> 00:06:53,580
computing Y hat and then using that to

115
00:06:49,380 --> 00:06:57,539
compute this and then back prop right

116
00:06:53,580 --> 00:07:01,800
doing that and now you have all these

117
00:06:57,539 --> 00:07:04,380
derivative terms and so you know W will

118
00:07:01,800 --> 00:07:07,889
get updated as some W minus the learning

119
00:07:04,380 --> 00:07:14,009
rate times DW right for each of the

120
00:07:07,889 --> 00:07:16,380
layers and similarly for B right now

121
00:07:14,009 --> 00:07:18,389
we've compute the back prop and have all

122
00:07:16,380 --> 00:07:20,550
these derivatives so that's one

123
00:07:18,389 --> 00:07:23,159
iteration of gradient descent for your

124
00:07:20,550 --> 00:07:24,949
neural network now before moving on just

125
00:07:23,159 --> 00:07:27,330
one more implementational detail

126
00:07:24,949 --> 00:07:32,090
conceptually will be useful to think of

127
00:07:27,330 --> 00:07:34,740
the cashier as storing the value of Z

128
00:07:32,090 --> 00:07:36,479
for the backward functions but when you

129
00:07:34,740 --> 00:07:37,740
implement this you see this in the

130
00:07:36,479 --> 00:07:38,190
programming exercise when you implement

131
00:07:37,740 --> 00:07:39,840
it

132
00:07:38,190 --> 00:07:42,029
you find that the cash may be a

133
00:07:39,840 --> 00:07:45,419
convenient way to get the value of the

134
00:07:42,029 --> 00:07:47,069
parameters at W 1 V 1 into the backward

135
00:07:45,419 --> 00:07:50,159
function as well so the program exercise

136
00:07:47,069 --> 00:07:55,669
you actually spawn the cash is Z as well

137
00:07:50,159 --> 00:07:58,949
as W and B all right so to store z2w to

138
00:07:55,669 --> 00:08:00,900
be to go from an implementational

139
00:07:58,949 --> 00:08:03,060
standpoint I just find this a convenient

140
00:08:00,900 --> 00:08:06,300
way to just you know get the parameters

141
00:08:03,060 --> 00:08:07,919
copied to where you need to need to use

142
00:08:06,300 --> 00:08:09,569
them later when you're computing back

143
00:08:07,919 --> 00:08:11,430
propagation so that's just an

144
00:08:09,569 --> 00:08:15,719
implementational detail that you see

145
00:08:11,430 --> 00:08:17,190
when you do the programming exercise so

146
00:08:15,719 --> 00:08:19,169
you've now seen one of the basic

147
00:08:17,190 --> 00:08:20,909
building blocks for implementing a deep

148
00:08:19,169 --> 00:08:22,349
neural network image layer there's a for

149
00:08:20,909 --> 00:08:24,000
propagation step and there's a

150
00:08:22,349 --> 00:08:24,780
corresponding backward propagation step

151
00:08:24,000 --> 00:08:26,970
and as

152
00:08:24,780 --> 00:08:29,400
cash deposit information from one to the

153
00:08:26,970 --> 00:08:31,110
other in the next video we'll talk about

154
00:08:29,400 --> 00:08:33,000
how you can actually implement these

155
00:08:31,110 --> 00:08:35,180
building blocks let's go into the next

156
00:08:33,000 --> 00:08:35,180
video