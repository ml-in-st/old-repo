1
00:00:00,030 --> 00:00:05,279
welcome back in this week's you learn to

2
00:00:02,639 --> 00:00:07,440
implement a neural network before diving

3
00:00:05,279 --> 00:00:09,059
into the technical details I wanted in

4
00:00:07,440 --> 00:00:10,889
this video to give you a quick overview

5
00:00:09,059 --> 00:00:13,679
of what you'll be seeing in this week's

6
00:00:10,889 --> 00:00:15,389
videos so if you don't follow all the

7
00:00:13,679 --> 00:00:17,100
details in this video don't worry about

8
00:00:15,389 --> 00:00:19,680
it we'll delve in the technical details

9
00:00:17,100 --> 00:00:21,660
in the next few videos but for now let's

10
00:00:19,680 --> 00:00:24,269
give a quick overview of how you

11
00:00:21,660 --> 00:00:26,250
implement in your network last week we

12
00:00:24,269 --> 00:00:30,300
had talked about logistic regression and

13
00:00:26,250 --> 00:00:32,430
we saw how this model corresponds to the

14
00:00:30,300 --> 00:00:35,520
following computation graph where you

15
00:00:32,430 --> 00:00:38,370
didn't put the features X and parameters

16
00:00:35,520 --> 00:00:40,620
W MB that allows you to compute Z which

17
00:00:38,370 --> 00:00:44,219
is then used to compute a and we were

18
00:00:40,620 --> 00:00:47,190
using a interchangeably with this output

19
00:00:44,219 --> 00:00:51,059
Y hat and then you can compute the loss

20
00:00:47,190 --> 00:00:52,920
function l a new network looks like this

21
00:00:51,059 --> 00:00:54,930
and as I'd already previously alluded

22
00:00:52,920 --> 00:00:57,239
you can form a neural network by

23
00:00:54,930 --> 00:01:00,420
stacking together a lot of little

24
00:00:57,239 --> 00:01:02,969
sigmoid units whereas previously this

25
00:01:00,420 --> 00:01:04,920
node corresponds to two steps of

26
00:01:02,969 --> 00:01:07,680
calculations the first three compute the

27
00:01:04,920 --> 00:01:11,640
Z value second is it computes this a

28
00:01:07,680 --> 00:01:14,549
value in this dual network this stack of

29
00:01:11,640 --> 00:01:17,880
notes will correspond to a Z like

30
00:01:14,549 --> 00:01:21,720
calculation like this as well as an a

31
00:01:17,880 --> 00:01:24,090
like calculation like that and then that

32
00:01:21,720 --> 00:01:26,790
node will correspond to another Z and

33
00:01:24,090 --> 00:01:29,040
another 8 like calculation so the

34
00:01:26,790 --> 00:01:30,000
notation which we should use later will

35
00:01:29,040 --> 00:01:32,759
look like this

36
00:01:30,000 --> 00:01:35,430
first what inputs the features X

37
00:01:32,759 --> 00:01:40,320
together with some parameters W and B

38
00:01:35,430 --> 00:01:42,930
and this will allow you to compute z1 so

39
00:01:40,320 --> 00:01:45,600
new notation that one should use is that

40
00:01:42,930 --> 00:01:48,689
we'll use a superscript square bracket 1

41
00:01:45,600 --> 00:01:50,759
to refer to quantities associated with

42
00:01:48,689 --> 00:01:53,579
this stack of nodes called a lair and

43
00:01:50,759 --> 00:01:56,280
then later we'll use superscript square

44
00:01:53,579 --> 00:01:58,920
bracket 2 to refer to quantities

45
00:01:56,280 --> 00:02:01,200
associated with Daniel really that's

46
00:01:58,920 --> 00:02:04,140
called another layer of the network and

47
00:02:01,200 --> 00:02:06,719
the superscript square brackets like we

48
00:02:04,140 --> 00:02:10,319
have here are not to be confused with

49
00:02:06,719 --> 00:02:12,390
the superscript round brackets which we

50
00:02:10,319 --> 00:02:14,080
used to refer to individual training

51
00:02:12,390 --> 00:02:16,300
examples so whereas X

52
00:02:14,080 --> 00:02:19,030
supersu round bracket I referred to the

53
00:02:16,300 --> 00:02:21,340
I've trained example superscript square

54
00:02:19,030 --> 00:02:25,570
bracket 1 and 2 refers to these

55
00:02:21,340 --> 00:02:28,600
different um layers layer 1 and layer 2

56
00:02:25,570 --> 00:02:32,860
in this network but they're going on

57
00:02:28,600 --> 00:02:35,350
after computing z1 similar to logistic

58
00:02:32,860 --> 00:02:39,000
regression there will be a computation

59
00:02:35,350 --> 00:02:44,550
to compute a 1 and that's just some

60
00:02:39,000 --> 00:02:49,270
sigmoid of z1 and then you compute Z 2

61
00:02:44,550 --> 00:02:54,610
using another linear equation and then

62
00:02:49,270 --> 00:02:57,370
compute a 2 and a 2 is the final output

63
00:02:54,610 --> 00:02:59,890
of the neural network and will also be

64
00:02:57,370 --> 00:03:01,390
used interchangeably with Y hat so I

65
00:02:59,890 --> 00:03:03,730
know there was a lot of details but the

66
00:03:01,390 --> 00:03:06,460
key intuition to take away is that

67
00:03:03,730 --> 00:03:09,220
whereas for logistic regression we had

68
00:03:06,460 --> 00:03:11,590
this Z followed by a calculation and

69
00:03:09,220 --> 00:03:13,780
this new network here we just do it

70
00:03:11,590 --> 00:03:16,390
multiple times as a CV followed by a

71
00:03:13,780 --> 00:03:19,959
calculation and a Z followed by a

72
00:03:16,390 --> 00:03:22,600
calculation and then you finally compute

73
00:03:19,959 --> 00:03:24,670
the loss at the end and you remember

74
00:03:22,600 --> 00:03:27,959
that for the just regression we had in

75
00:03:24,670 --> 00:03:30,970
some backward calculation in order to

76
00:03:27,959 --> 00:03:34,750
compute derivatives are so confusing

77
00:03:30,970 --> 00:03:36,580
da easy and so on so in the same way in

78
00:03:34,750 --> 00:03:38,860
a new network we'll end up doing a

79
00:03:36,580 --> 00:03:44,910
backward calculation that looks like

80
00:03:38,860 --> 00:03:50,890
this and we jump you end up computing da

81
00:03:44,910 --> 00:03:57,850
2 DZ 2 that allows you to compute so DW

82
00:03:50,890 --> 00:04:01,090
2 DB 2 and so on in this order the right

83
00:03:57,850 --> 00:04:05,020
to less backward calculation that is

84
00:04:01,090 --> 00:04:05,360
denoting with the red arrows so thank

85
00:04:05,020 --> 00:04:07,970
you

86
00:04:05,360 --> 00:04:09,770
quick overview of what a new network

87
00:04:07,970 --> 00:04:12,950
websites based you take a logistic

88
00:04:09,770 --> 00:04:14,630
regression and repeating it twice I know

89
00:04:12,950 --> 00:04:16,880
there was a lot of new notation lot of

90
00:04:14,630 --> 00:04:18,980
new details don't worry about to get and

91
00:04:16,880 --> 00:04:20,900
follow everything we'll go into the

92
00:04:18,980 --> 00:04:22,820
details most slowly in the next few

93
00:04:20,900 --> 00:04:24,620
videos so let's go on to the next video

94
00:04:22,820 --> 00:04:27,850
we'll stop to talk about the neural

95
00:04:24,620 --> 00:04:27,850
network representation