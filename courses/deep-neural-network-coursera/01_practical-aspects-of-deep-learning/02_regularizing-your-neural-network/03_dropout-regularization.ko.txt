L2 일반화에 추가로, 더욱 강력한 일반화 테크닉인 droupout이라는 것이 있습니다. 어떻게 작동하는 것인지 알아보겠습니다. 왼쪽과 같은 신경망을 트레이닝 하는데 overfitting 하는 경우입니다. 이런 경우 dropout으로 하는 것인 이와 같습니다. 여기에다가 신경망을 복사해보겠습니다. dropout에서는 무엇을 할 것이냐면, 이 네트워크의 각각 층별로 살펴보면서 신경망의 노드를 제거하는 확률을 세팅해볼 것입니다. 여기 각각의 층 별로, 각각의 노드별로, 동전을 던져서 노드를 유지할 것인지 제거할 것인지 각각 0.5 확률이 있다고 할 것입니다. 동전을 던진 후에, 여기 노드들을 제거하겠다고 할 수도 있습니다. 그러면, 여기서 하는 것은 들어오고 나가는 링크 또한 제거를 같이 할 것입니다. 그렇게 되면 훨씬 더 작은, 감소된 네트워크가 남을 것입니다. 그 다음에 후 방향전파 트레이닝을 진행합니다. 여기처럼 감소된 네트워크에 예제가 있고, 또 다른 예제가 있는데요, 이 경우에는 동전을 몇개 던진 이후에 노드의 뭉치를 유지하거나 제거하는 방법이 있습니다. 각각의 트레이닝 예시에 대해서, 여기 있는 방법 중에서 신경만 트레이닝을 시킬 것입니다. 아마도 조금 미친 테크닉이라고 생각할 수도 있는데요, 이것을 코딩하는 방법이 무작위로 이루어지지만, 실제로 작동을 합니다. 각각의 예시에 대해서 훨씬 더 작은 네트워크를 트레이닝 시키기 때문에, 또는 네트워크를 일반화시킬 수 있는지에  대해 감을 잡을 수 있을 수도 있습니다, 이런 훨씬 더 작은 네트워크가 트레이닝되고 있기 때문이죠. dropout을 어떻게 도입하는지 알아보겠습니다. dropout을 도입하는 방법은 몇가지가 있습니다. 가장 흔한 방법을 보여드리겠습니다. 바로 inverted dropout이라고 하는 기술입니다. 완성도를 위해서, 이것을 l=3이라는 층으로 묘사한다고 해봅시다. 코딩할때는 이것을 몇개의 "3"들로 적을텐데요, 싱글 레이어에서 dropout을 묘사하는 방법을 보여드리고 있습니다. 여기서 할 것은, vector d를 설정해서, d^3는 3이라는 층에대한 dropout vector일 것인데요, d 3이라는 것은 np.random.rand(a) 입니다. 이것은 a3와 같은 모양일 것입니다. 저는 이것이 어떤 숫자보다 작은지 확인할 것입니다. 그걸 keep.prob이라 부를 텐데요. 그러면 keep.prob은 숫자가 됩니다. 이전에는 0.5였는데요, 이번 예제에서는 0.8을 이용하겠습니다. 또한, 특정 숨겨진 유닛이 유지될 확률이 있을 것입니다. 그러므로 keep.prob 은 0.8입니다. 이러면 0.2는 숨겨진 유닛을 제거할 확률입니다. 이것이 하는 것은 random matrix를 생성합니다. 만약 인수분해를 안 한 경우 헤는 이것도 됩니다. d3는 그러면 매트릭스일 것입니다. 각각의 예시이나 숨겨진 유닛에 대해서 d3가 1이 될 확률이 0.8 해당하는 경우을 가르킵니다. 또, 0이 될 확률이 20퍼센트인 경우를 가르키죠. 0.8보다 작은 랜덤 숫자가 0.8의 확률로 1이거나 참일 확률입니다. 20퍼센트 또는 0.2의 확률로 거짓일 것입니다. 0이 되는 경우를 말하죠. 그 다음으로 할 일은, 세번쩨 층에서 activation을 가집니다. 밑에 예제에서 a3라고 부르겠습니다. 그러면 a3는 산출하는 activation이 있습니다. 그리고 a3를 이전의 a3 곱하기 이거는 element wise multiplication인데요, 이것은 또한 a3 곱하기는 d3와 동일하다고 적을 수 있습니다. 이것이 하는 것은 d3의 모든 요소가 0인 경우, 이런 경우 즉, 각각의 element가 0인 경우는 20퍼센트 확률이었는데요, 여기 이 multiply operation은 d3의 요소들을 0으로 만드는 역할을 합니다. 이것을 파이썬에서 하면, 엄밀히 이야기하면 d3가 boolean array 가 됩니다. 그 값이 참, 거짓은 경우를 얘기하죠, 0과 1의 값 대신에요. 그러나 multiply operation은 잘 작동하기 때문에 참, 거짓의 값을 0과 1로 이해하도록 하겠습니다. 이 과정을 직접 파이썬에서 시도해보시면 여러분이 확인할 수 있습니다. 마지막으로, 우리는 a3를 가지고 확장을 할 것입니다. 마지막으로, 우리는 a3를 가지고 확장을 할 것입니다. 이 마지막 역할이 하는 것을 설명하겠습니다. 토론의 목표를 위해서, 50 단위 또는 50개의 신경세포가 세번째 층에 있다고 가정해보겠습니다. 그러면 a3가 50 x 1차원 또는 인수분해를 하는 경우, 50 x m 차원일 수 있겠습니다. 그럼 80퍼센트의 확률로 유지하거나, 20퍼센트 확률로 제거하는 경우의 수가 있다고 해봅시다. 이 뜻은, 평균적으로 10 유닛은 닫거나, 10유닛이 0으로 된다는 뜻과 같습니다. 그러면 이제 z^4의 값을 보면, z^4 의 값은 w^4 * a^3 + b^4와 같을 것입니다. 그러면, 예상되는 것은, 이것이 20퍼센까지 줄어들을 것입니다. 무슨 뜻이냐 면, 20퍼센트의 a3의 element가 0이 될 것이라는 겁니다. 그러면, z^4의 기대 값을 감소하지 않게 하기 위해서는 이것을 가져야 합니다. 그리고 0.8로 나눕니다. 왜냐면, 이게 조정되거나 다시 20퍼센트만큼 올라갈 수 있기 때문입니다. 그러면 a3의 기대 값이 변하지 않았습니다. 여기 이 라인은 inverted dropout technique이라고 하는 것인데요, 이것의 효과는, keep.prob을 어떻게 설정하더라도, 0.8이던 0.9이던, 심지어 1이더라도 만약 1로 설정이 되면 dropout은 없습니다. 0.5 이던 어떤 값이라도 모두 유지하기 때문이죠. inverted dropout technique은 keep.prob으로 나누면서 a3의 기대값이 동일하게 유지하도록 해줍니다. 그리고 테스트를 하는 경우, 신경망을 평가할 때에, 다음 슬라이드에서 이야기하겠지만, 여기 이 inverted dropout technique은 제가 여기 이렇게 초록색 박스를 그린 이 부분이 바로 해당하는 부분인데요, 이것이 test time을 쉽게 만들어줍니다. Scaling 문제가 덜하기 때문이죠. 제가 아는 선에서는, 가장 흔한 dropout의 도입방식은 바로 inverted dopout 방식입니다. 저는 여러분께 이것을 도입하길 권장드립니다. 하지만 이전에 dropout에서 반복 테스트 업무를 하던 때에, 여기 이 keep.prob 라인 부분을 놓친 적이 있었습니다. 그렇게 해서 test time에서 평균이 더욱 더 점차 복잡해지는 것이죠. 하지만 다른 버전들을 이제는 잘 쓰지 않습니다. 그러면 여러분은 d vector를 사용해서 여러분도 느끼시겠지만 트레이닝 예시의 유형에 따라 서로 다른 숨겨진 유닛을 0으로 만듭니다. 동일한 트레이닝 세트에서 mulltiple pass를 진행하면, 트레이닝 세트에 통과하는 다른 pass들에 따라서, 다른 숨겨진 유닛을 무작위로 0으로 만들 것입니다. 꼭 동일한 숨겨진 유닛을 0으로 만들고 한 기울기 강하 수행절차에서 말이죠, 그리고 또 다른 2번째의 기울기 강하에서 트레이닝 세트를 2번째로 통과시켜서 또 다른 패턴이 숨겨진 유닛을 0으로 만들어야 하는 것이 아닙니다. 3번째 층의 vector d 또는 d3는 어떤 것을 0으로 만들지 결정하는데 사용됩니다. 전 방향전파과 back prop 2개 모두에서 말이죠. 여기서는 전 방향전파만 보여주고 있습니다. test time에서 알고리즘을 트레이닝 했으니, 이제 이렇게 합니다. test time에서는 x라는 것이 주어지고, 또는 예측하고 싶어하는 상대가 주어집니다. 그리하여 표준 노테이션을 사용해서 여기서 a^0를 사용하겠습니다. 0개 층의 activation을 나타내는데요, 테스트 예시 x를 나타냅니다. 저희가 할 것은 테스트 타임에 dropout을 사용하지 않을 것입니다. 특히, Z^1= w^1.a^0 + b^1을 말이죠. a1원  g^1(z^1 Z)이구요. z2는 w^2.a^1 + b^2 입니다. a2 는 등등 말이죠. 마지막 층 예측수치 ŷ까지 내려갑니다. 확인 하실 수 있는 것인, dropout을 여기서 명백히 사용하지 않고, 동전을 무작위로 던지지 않고, 어떤 숨겨진 유닛을 제거할지 동전을 던지지 않는다는 것입니다. 그 이유는 test time에서 예측을 하는 경우, 여러분의 결과값이 임의의 숫자가 되면 좋지 않기 때문입니다. 만약 여러분이 dropout을 테스트타임에 도입한다 하면, 그것은 예측수치에 noise를 더할 뿐입니다. 이론적으로, 여러분이 할 수 있는 것은 예측을 몇 번이고 진행하는 것입니다. 임의로 dropout된 숨겨진 유닛을 가지고 말이죠. 하지만 산출하는 이런 과정이 효율적이지 않고, 어렴풋이 비슷한 결과값을 줄 것입니다. 진행절차는 다르지만 결과는 아주 아주 비슷할 것입니다. 언급하자면, inverted dropout 은, 이전 라인에서 진행한 것을 기억하시겠지만, 저희는 keep.prob으로 나눴었습니다. 이 효과는 여러분이 테스트타임에 scaling에서 dropout을 도입하지 않더라도 여기 이런 activation의 기대값은 변하지 않습니다. 그러므로 여러분은 이렇게 쌩뚱맞은 부가적인 scaling 매개 변수를 넣을 필요가 없습니다. 이것은 여러분이 training time에서 있었던 것과는 다릅니다. 이게 dropout에 관한 내용인데요, 이번주 학습에 이 내용을 적용하면, 처음으로 경험을 얻을 수 있을 것입니다. 하지만 이것이 왜 어떻게 해서 작동하는 것일까요? 다음 비디오에서는 dropout이 정확히 무엇을 하는지 조금 더 직관적인 부분을 알아보겠습니다. 다음 비디오로 넘어가겠습니다.