1
00:00:00,000 --> 00:00:01,230
In the last video,

2
00:00:01,230 --> 00:00:03,720
you saw the equations for back propagation.

3
00:00:03,720 --> 00:00:06,900
In this video, let's go over some intuition using

4
00:00:06,900 --> 00:00:10,515
the computation graph for how those equations were derived.

5
00:00:10,515 --> 00:00:12,385
This video is completely optional.

6
00:00:12,385 --> 00:00:14,106
So, feel free to watch or not.

7
00:00:14,106 --> 00:00:16,360
You should be able to do the whole work either way.

8
00:00:16,360 --> 00:00:19,410
So, recall that when we talk about logistic regression,

9
00:00:19,410 --> 00:00:23,685
we had this forward pass where we compute Z,

10
00:00:23,685 --> 00:00:26,145
then A and then the loss.

11
00:00:26,145 --> 00:00:27,445
And then to take the derivatives,

12
00:00:27,445 --> 00:00:32,520
we had this backward pass where we could first compute DA,

13
00:00:32,520 --> 00:00:35,400
and then go on to compute DZ,

14
00:00:35,400 --> 00:00:40,720
and then go on to compute DW and DB.

15
00:00:40,720 --> 00:00:46,970
So, the definition for the loss was L of A,

16
00:00:46,970 --> 00:00:52,655
Y equals negative Y log A minus one,

17
00:00:52,655 --> 00:00:57,440
minus Y times log one minus A.

18
00:00:57,440 --> 00:00:59,750
So, if you are familiar with

19
00:00:59,750 --> 00:01:03,600
calculus and you take the derivative of this with respect to A,

20
00:01:03,600 --> 00:01:06,156
that would give you the formula for DA.

21
00:01:06,156 --> 00:01:09,060
So, DA is equal to that.

22
00:01:09,060 --> 00:01:12,750
And if we actually figure out the calculus you could show that this is

23
00:01:12,750 --> 00:01:18,808
negative Y over A plus one minus Y over one minus A.

24
00:01:18,808 --> 00:01:23,040
You just kind of divide that from calculus by taking derivatives of this.

25
00:01:23,040 --> 00:01:26,680
It turns out when you take another step backwards to compute DZ,

26
00:01:26,680 --> 00:01:32,430
we did work out that DZ is equal to A minus Y. I did explain why previously,

27
00:01:32,430 --> 00:01:37,920
but it turns out that from the chamber of calculus DZ is equal

28
00:01:37,920 --> 00:01:45,425
to DA times G prime of Z.

29
00:01:45,425 --> 00:01:50,535
Where here G of Z equals sigmoid of Z

30
00:01:50,535 --> 00:01:56,245
is our activation function for this output unit in logistic regression, right?

31
00:01:56,245 --> 00:02:00,570
So, just remember this is still logistic regression where we have X1, X2,

32
00:02:00,570 --> 00:02:05,757
X3 and then just one sigmoid unit and that gives us A,

33
00:02:05,757 --> 00:02:07,400
will gives us Y end.

34
00:02:07,400 --> 00:02:11,400
So, here are the activation function was a sigmoid function.

35
00:02:11,400 --> 00:02:12,960
And as an aside,

36
00:02:12,960 --> 00:02:17,205
only for those of you familiar with the chamber of calculus

37
00:02:17,205 --> 00:02:22,520
the reason for this is because A is equal to sigmoid of Z.

38
00:02:22,520 --> 00:02:29,310
And so, partial of L with respect to Z is equal to partial of

39
00:02:29,310 --> 00:02:36,800
L with respect to A times DA, DZ.

40
00:02:36,800 --> 00:02:39,611
This is A is equal to sigmoid of Z,

41
00:02:39,611 --> 00:02:42,970
this is equal to DDZ,

42
00:02:42,970 --> 00:02:49,080
G of Z, which is equal to G prime of Z.

43
00:02:49,080 --> 00:02:54,060
So, that's why this expression which is DZ in our code is equal

44
00:02:54,060 --> 00:02:59,484
to this expression which is DA in our code times G prime of Z.

45
00:02:59,484 --> 00:03:05,860
And so this is just that.

46
00:03:05,860 --> 00:03:09,172
So, that last derivation would made sense only if

47
00:03:09,172 --> 00:03:13,510
you're familiar with calculus and specifically the chamber from calculus.

48
00:03:13,510 --> 00:03:15,325
But if not don't worry about it.

49
00:03:15,325 --> 00:03:18,853
I'll try to explain the intuition wherever it's needed.

50
00:03:18,853 --> 00:03:22,315
And then finally having computed DZ for this regression,

51
00:03:22,315 --> 00:03:26,335
we will compute DW which turns out was

52
00:03:26,335 --> 00:03:31,470
DZ times X and DB which is just DZ when you have a single training example.

53
00:03:31,470 --> 00:03:33,822
So, that was logistic regression.

54
00:03:33,822 --> 00:03:36,700
So, what we're going to do when computing back

55
00:03:36,700 --> 00:03:40,090
propagation for a neural network is a calculation a lot like

56
00:03:40,090 --> 00:03:46,995
this but only we'll do it twice because now we have not X going to an output unit,

57
00:03:46,995 --> 00:03:50,930
but X going to a hidden layer and then going to an output unit.

58
00:03:50,930 --> 00:03:58,405
And so instead of this computation being sort of one step as we have here,

59
00:03:58,405 --> 00:04:04,483
we'll have you two steps here in this kind of a neural network with two layers.

60
00:04:04,483 --> 00:04:08,586
So, in this two layer neural network that is we have the input layer,

61
00:04:08,586 --> 00:04:10,138
a hidden layer and then output layer.

62
00:04:10,138 --> 00:04:12,070
Remember the steps of a computation.

63
00:04:12,070 --> 00:04:17,210
First you compute Z1 using this equation,

64
00:04:17,210 --> 00:04:22,177
and then compute A1 and then you compute Z2.

65
00:04:22,177 --> 00:04:25,505
And notice Z2 also depends on the parameters W2 and B2.

66
00:04:25,505 --> 00:04:27,530
And then based on Z2,

67
00:04:27,530 --> 00:04:32,815
compute A2 and then finally that gives you the loss.

68
00:04:32,815 --> 00:04:41,560
What backpropagation does is it will go backward to compute DA2 and then DZ2,

69
00:04:41,560 --> 00:04:48,805
and then you go back to compute DW2 and DP2,

70
00:04:48,805 --> 00:04:53,232
go backwards to compute DA1,

71
00:04:53,232 --> 00:04:57,278
DZ1 and so on.

72
00:04:57,278 --> 00:05:00,290
We don't need to take the riveter as respect to

73
00:05:00,290 --> 00:05:03,745
the input X since the input X for supervised learning suffix.

74
00:05:03,745 --> 00:05:07,845
We're not trying to optimize X so we won't bother to take the riveters.

75
00:05:07,845 --> 00:05:09,655
At least, for supervised learning,

76
00:05:09,655 --> 00:05:15,605
we respect X. I'm going to skip explicitly computing DA2.

77
00:05:15,605 --> 00:05:18,110
If you want, you can actually compute

78
00:05:18,110 --> 00:05:20,750
DA2 and then use that to compute DZ2 but, in practice,

79
00:05:20,750 --> 00:05:25,760
you could collapse both of these steps into one step so you end up

80
00:05:25,760 --> 00:05:31,715
at DZ2= A2-Y, same as before.

81
00:05:31,715 --> 00:05:33,620
And, you have also,

82
00:05:33,620 --> 00:05:38,615
I'm going to write DW2 and DB2 down here below.

83
00:05:38,615 --> 00:05:46,700
You have that DW2=DZ2*A1,

84
00:05:46,700 --> 00:05:52,040
transpose, and DB2=DZ2.

85
00:05:52,040 --> 00:05:55,990
This step is quite similar for logistic regression where we had

86
00:05:55,990 --> 00:06:03,550
that DW=DZ*X except that now,

87
00:06:03,550 --> 00:06:08,770
A1 plays the role of X and there's an extra transpose there because the

88
00:06:08,770 --> 00:06:14,125
relationship between the capital matrix W and our individual parameters W,

89
00:06:14,125 --> 00:06:16,660
there's a transpose there, right?

90
00:06:16,660 --> 00:06:24,370
Because W=[---] in the case of the logistic regression with a single output.

91
00:06:24,370 --> 00:06:26,980
DW2 is like that, whereas,

92
00:06:26,980 --> 00:06:32,440
W here was a column vector so that's why it has an extra transpose for A1,

93
00:06:32,440 --> 00:06:36,980
whereas, we didn't for X here for logistic regression.

94
00:06:36,980 --> 00:06:40,335
This completes half of backpropagation.

95
00:06:40,335 --> 00:06:44,045
Then, again, you can compute DA1 if you wish.

96
00:06:44,045 --> 00:06:49,440
Although, in practice, the computation for DA1 and

97
00:06:49,440 --> 00:06:52,330
the DZ1 are usually collapsed into one step and so

98
00:06:52,330 --> 00:06:57,130
what you'll actually implement is that DZ1=W2,

99
00:06:57,130 --> 00:07:03,480
transpose *DZ2, and then times an element

100
00:07:03,480 --> 00:07:10,383
Y's product of G1 prime of Z1.

101
00:07:10,383 --> 00:07:13,960
And, just to do a check on the dimensions, right?

102
00:07:13,960 --> 00:07:19,510
If you have a new network that looks like this,

103
00:07:19,510 --> 00:07:23,000
I'll put Y if so.

104
00:07:23,000 --> 00:07:28,265
If you have N0, NX=N0 input features,

105
00:07:28,265 --> 00:07:30,230
N1 head in units,

106
00:07:30,230 --> 00:07:34,275
and N2 so far.

107
00:07:34,275 --> 00:07:36,740
N2, in our case,

108
00:07:36,740 --> 00:07:38,565
just one output unit,

109
00:07:38,565 --> 00:07:48,795
then the matrix W2 is (N2,N1) dimensional,

110
00:07:48,795 --> 00:07:57,490
Z2 and therefore DZ2 are going to be (N2,N1) by one dimensional.

111
00:07:57,490 --> 00:07:59,850
This really is going to be a one by one when we are doing binary classification,

112
00:07:59,850 --> 00:08:04,750
and Z1 and therefore also

113
00:08:04,750 --> 00:08:10,045
DZ1 are going to be N1 by one dimensional, right?

114
00:08:10,045 --> 00:08:16,115
Note that for any variable foo and D foo always have the same dimension.

115
00:08:16,115 --> 00:08:20,850
That's why W and DW always have the same dimension and similarly,

116
00:08:20,850 --> 00:08:23,680
for B and DB and Z and DZ and so on.

117
00:08:23,680 --> 00:08:26,895
To make sure that the dimensions of this all match up,

118
00:08:26,895 --> 00:08:35,430
we have that DZ1=W2 transpose times DZ2

119
00:08:35,430 --> 00:08:44,490
and then this is an element Y's product times G1 prime of Z1.

120
00:08:44,490 --> 00:08:47,040
Matching the dimensions from above,

121
00:08:47,040 --> 00:08:52,575
this is going to be N1 by one=W2 transpose,

122
00:08:52,575 --> 00:08:57,945
we transpose of this so there's going to be N1 by N2 dimensional.

123
00:08:57,945 --> 00:09:05,790
DZ2 is going to be N2 by one dimensional and then this,

124
00:09:05,790 --> 00:09:07,230
this is the same dimension as Z1.

125
00:09:07,230 --> 00:09:11,820
This is also N1 by one dimensional so element Y's product.

126
00:09:11,820 --> 00:09:14,350
The dimensions do make sense, right?

127
00:09:14,350 --> 00:09:18,330
N1 by one dimensional vector can be obtained by

128
00:09:18,330 --> 00:09:23,520
N1 by N2 dimensional matrix times N2 by N1 because the product of

129
00:09:23,520 --> 00:09:28,890
these two things gives you an N1 by one dimensional matrix and so this becomes

130
00:09:28,890 --> 00:09:34,618
the element Y's product of two N1 by one dimensional vectors,

131
00:09:34,618 --> 00:09:36,060
and so the dimensions do match.

132
00:09:36,060 --> 00:09:40,620
One tip when implementing a back prop.

133
00:09:40,620 --> 00:09:44,790
If you just make sure that the dimensions of your matrices match up,

134
00:09:44,790 --> 00:09:47,190
so you think through what are the dimensions of

135
00:09:47,190 --> 00:09:50,430
the various matrices including W1, W2, Z1,

136
00:09:50,430 --> 00:09:54,180
Z2, A1, A2 and so on and just make sure

137
00:09:54,180 --> 00:09:58,642
that the dimensions of these matrix operations match up,

138
00:09:58,642 --> 00:10:03,390
sometimes that will already eliminate quite a lot of bugs in back prop.

139
00:10:03,390 --> 00:10:06,960
All right. This gives us the DZ1 and then finally,

140
00:10:06,960 --> 00:10:12,160
just to wrap up DW1 and DB1,

141
00:10:12,160 --> 00:10:13,965
we should write them here I guess,

142
00:10:13,965 --> 00:10:17,200
but since I'm running of the space right on the right of the slight,

143
00:10:17,200 --> 00:10:21,965
DW1 and DB1 are given by the following formulas.

144
00:10:21,965 --> 00:10:25,950
This is going to be equal to the DZ1 times X transpose

145
00:10:25,950 --> 00:10:28,905
and this is going to be equal to DZ.

146
00:10:28,905 --> 00:10:34,045
You might notice a similarity between these equations and these equations,

147
00:10:34,045 --> 00:10:37,095
which is really no coincidence because X

148
00:10:37,095 --> 00:10:41,660
plays the role of A0 so X transpose is A0 transpose.

149
00:10:41,660 --> 00:10:45,484
Those equations are actually very similar.

150
00:10:45,484 --> 00:10:50,260
That gives a sense for how backpropagation is derived.

151
00:10:50,260 --> 00:10:54,530
We have six key equations here for DZ2, DW2,

152
00:10:54,530 --> 00:11:00,190
DB2, DZ1,DW1 and D1.

153
00:11:00,190 --> 00:11:05,767
Let me just take these six equations and copy them over to the next slide. Here they are.

154
00:11:05,767 --> 00:11:08,950
So far, we have to write backpropagation,

155
00:11:08,950 --> 00:11:13,959
for if you are training on a single training example at the time,

156
00:11:13,959 --> 00:11:21,530
but it should come as no surprise that rather than working on a single example at a time,

157
00:11:21,530 --> 00:11:27,810
we would like to vectorize across different training examples.

158
00:11:27,810 --> 00:11:30,971
We remember that for propagation,

159
00:11:30,971 --> 00:11:33,545
when we're operating on one example at a time,

160
00:11:33,545 --> 00:11:41,665
we had equations like this as was say A1=G1 of Z1.

161
00:11:41,665 --> 00:11:43,655
In order to vectorize,

162
00:11:43,655 --> 00:11:51,260
we took say the Zs and stacked them up in

163
00:11:51,260 --> 00:12:00,775
columns like this onto Z1M and call this capital Z.

164
00:12:00,775 --> 00:12:04,960
Then we found that by stacking things up in columns

165
00:12:04,960 --> 00:12:10,240
and defining the capital uppercase version of this,

166
00:12:10,240 --> 00:12:17,093
we then just had Z1=W1 X + B

167
00:12:17,093 --> 00:12:24,700
and A1=G1 of Z1, right?

168
00:12:24,700 --> 00:12:28,645
We define the notation very carefully in this course to make sure that

169
00:12:28,645 --> 00:12:35,550
stacking examples into different columns of a matrix makes all this work out.

170
00:12:35,550 --> 00:12:40,105
It turns out that if you go through the math carefully,

171
00:12:40,105 --> 00:12:46,645
the same trick also works for backpropagation so the vectorize equations are as follows.

172
00:12:46,645 --> 00:12:52,250
First, if you take these DZs for different training examples and stack

173
00:12:52,250 --> 00:12:58,339
them as the different columns of a matrix and the same for this and the same for this,

174
00:12:58,339 --> 00:13:03,070
then this is the vectorize implementation and then here's the definition for,

175
00:13:03,070 --> 00:13:05,569
or here's how you can compute DW2.

176
00:13:05,569 --> 00:13:11,130
There is this extra 1/M because the cost function J is

177
00:13:11,130 --> 00:13:18,410
this 1/M of sum for Y = one through M of the losses.

178
00:13:18,410 --> 00:13:20,615
When computing the riveters,

179
00:13:20,615 --> 00:13:23,885
we have that extra 1/M term just as we did when we were

180
00:13:23,885 --> 00:13:27,982
computing the wait up days for the logistic regression.

181
00:13:27,982 --> 00:13:31,790
That's the update you get for DB2.

182
00:13:31,790 --> 00:13:40,640
Again, some of the DZs and then with a 1/M and then DZ1 is computed as follows.

183
00:13:40,640 --> 00:13:49,109
Once again, this is an element Y's product only whereas previously,

184
00:13:49,109 --> 00:13:56,595
we saw on the previous slide that this was an N1 by one dimensional vector.

185
00:13:56,595 --> 00:14:03,185
Now, this is a N1 by M dimensional matrix.

186
00:14:03,185 --> 00:14:09,045
Both of these are also N1 by M dimensional.

187
00:14:09,045 --> 00:14:19,310
That's why that asterisk is element Y's product and then finally,

188
00:14:19,310 --> 00:14:21,454
the remaining two updates.

189
00:14:21,454 --> 00:14:25,836
Perhaps it shouldn't look too surprising.

190
00:14:25,836 --> 00:14:29,510
I hope that gives you some intuition for how the backpropagation algorithm is derived.

191
00:14:29,510 --> 00:14:32,205
In all of machine learning,

192
00:14:32,205 --> 00:14:34,820
I think the derivation of the backpropagation algorithm

193
00:14:34,820 --> 00:14:38,465
is actually one of the most complicated pieces of math I've seen,

194
00:14:38,465 --> 00:14:42,470
and it requires knowing both linear algebra as well as

195
00:14:42,470 --> 00:14:46,830
the derivative of matrices to re-derive it from scratch from first principles.

196
00:14:46,830 --> 00:14:50,165
If you are an expert in matrix calculus,

197
00:14:50,165 --> 00:14:54,255
using this process, you might prove the derivative algorithm yourself,

198
00:14:54,255 --> 00:14:57,500
but I think there are actually plenty of deep learning practitioners

199
00:14:57,500 --> 00:15:01,060
that have seen the derivation at about the level you've

200
00:15:01,060 --> 00:15:04,100
seen in this video and are already able to have

201
00:15:04,100 --> 00:15:08,580
all the very intuitions and be able to implement this algorithm very effectively.

202
00:15:08,580 --> 00:15:10,070
If you are an expert in calculus,

203
00:15:10,070 --> 00:15:13,395
do see if you can derive the whole thing from scratch.

204
00:15:13,395 --> 00:15:15,665
It is one of the very hardest pieces of math.

205
00:15:15,665 --> 00:15:20,010
One of the very hardest derivations that I've seen in all of machine learning.

206
00:15:20,010 --> 00:15:22,861
Either way, if you implement this,

207
00:15:22,861 --> 00:15:27,260
this will work and I think you have enough intuitions to tune and get it to work.

208
00:15:27,260 --> 00:15:30,830
There's just one last detail I want to

209
00:15:30,830 --> 00:15:34,190
share with you before you implement your neural network,

210
00:15:34,190 --> 00:15:37,720
which is how to initialize the weights of your neural network.

211
00:15:37,720 --> 00:15:40,600
It turns out that initializing your parameters,

212
00:15:40,600 --> 00:15:42,560
not to zero but randomly,

213
00:15:42,560 --> 00:15:45,515
turns out to be very important for training your neural network.

214
00:15:45,515 --> 00:15:48,000
In the next video, you'll see why.