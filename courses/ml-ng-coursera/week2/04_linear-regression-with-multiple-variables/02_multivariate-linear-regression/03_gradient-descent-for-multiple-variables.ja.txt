前回のビデオでは、線形回帰のうち、 複数のフィーチャー、または変数に対する物を議論した。 このビデオでは、その仮説にどうパラメータをフィットさせるかを議論しよう。 特に、最急降下法（Gradient Descent）を複数フィーチャーの線形回帰に どう使うかを議論していきたい。 我らのノーテーションを簡単に要約すると、これが複数変数における線形回帰の 我らのフォーマルな仮説で、そこではx0=1のコンベンションを採用する。 このモデルのパラメータはシータ0からシータnまでだが、 これをn個の別々のパラメータと考える代わりに、そう考えてもいいのだが、その代わりにパラメータを シータというn+1次元のベクトルと考える事としよう。 つまり、このモデルのパラメータ自身も ベクトルと考える。 コスト関数はJのシータ0からシータnまでで、 普通の誤差項の二乗和で与えられる。
だがJをこれらn+1個の数の 関数と考える代わりに、より一般的にJを単なるパラメータベクトル、シータの 関数とみなす。つまりここのシータはベクトル。 最急降下法はこんな感じ。
各シータjを シータj - アルファ×この微分項 で、 繰り返し何度も更新していく。 そしてこれを、Jのシータと書く。
シータjが、シータj マイナスの 学習率アルファ掛ける事の、コスト関数の微分、、、 正確にはパラメータであるシータ jによる偏微分。 最急降下法を実装する時にこれがどんな感じか、 特に偏微分の項がどんな感じか見ていこう。 N=1のフィーチャーの時に最急降下法を用いると、こうなる。 パラメータ シータ0とシータ1に別々の2つのアップデートのルールがあった。 これは今やお馴染みだろう。
そしてこの項はもちろん、 パラメータ シータ0によるコスト関数の偏微分だ。 同様に似たようなアップデートルールがシータ1についてもあった。 ちょっとだけ違うのは、一つしかフィーチャーが無かった時は そのフィーチャーをx(i)と呼べたが、この新しいノーテーションでは、 we would of course call this 
x(i)<u>1 to denote our one feature.</u> つまりそれは一つしかフィーチャーを持たない場合だ。 では新しいアルゴリズムを、フィーチャーが一つより多い場合について見てみよう。 つまりフィーチャーの数nが1より大きい場合もありうる、というケース。 最急降下法のアップデートルールはこうなる。
解析学が分かる人の為に言っておくと、 コスト関数の定義をとってきて、 そのコスト関数Jをパラメータ シータiに関して偏微分を取ると、 その偏微分の項はここに青い箱で書いた項と 正確に一致する事が分かる。 そしてもしこれを実装すれば、それは多変量の線形回帰に対する 最急降下法の、動く実装とあいなる。 このスライドで行いたい最後の仕事は、 この新しいアルゴリズムと古いアルゴリズムは同じ事というなんとなくの感覚を感じてもらいたい、という事。 言い換えると両者が何故似たアルゴリズムなのか、どうしてどちらも最急降下法アルゴリズムなのかを感覚的に分かって欲しい。 2つのフィーチャー、または2つ以上のフィーチャーがあるケースを 考えてみよう。
例えばシータ0 シータ1 シータ2の、 3つのアップデートルールがある。
さらなる別のシータもあっても良い。 ここでシータ0のアップデートルールを見ると、 これは前にやった、n=1の時のアップデートと 同じ事に気付く。 それらが等しい理由は、もちろん、 because in our notational convention we
had this x(i)<u>0 = 1 convention, which is</u> そんな訳で、マゼンダの箱で描いたこれら二項は等価である。 同じように、シータ1のアップデートルールを見ると、 ここのこの項は以前のシータ1の時の物、 つまり以前の方程式またはアップデートルールと等価だ。 where of course we're just using
this new notation x(i)<u>1 to denote</u> 今は一つより多いフィーチャーを扱っている。 シータ2などにも似たようなアップデートルールを用いる事が出来る。 このスライドではたくさんの事を説明したので、ビデオを一旦一時停止して、 このスライドの全ての数式をゆっくりと見直して、 ここにある事を全てしっかりと理解していることを確認する事を、激しく推奨する。 だがもしここに書かれたアルゴリズムを実装したら、 複数フィーチャーの線形回帰の、動く実装を得る事になるよ。