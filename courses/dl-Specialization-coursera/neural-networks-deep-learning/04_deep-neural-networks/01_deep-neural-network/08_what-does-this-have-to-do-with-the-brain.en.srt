1
00:00:00,060 --> 00:00:04,259
so what a deep learning have to do the

2
00:00:02,370 --> 00:00:06,480
brain at the risk of giving away the

3
00:00:04,259 --> 00:00:08,670
punchline I would say not a whole lot

4
00:00:06,480 --> 00:00:10,469
but let's take a quick look at why

5
00:00:08,670 --> 00:00:13,530
people keep making the analogy between

6
00:00:10,469 --> 00:00:15,480
deep learning and the human brain when

7
00:00:13,530 --> 00:00:18,600
you implement a neural network this is

8
00:00:15,480 --> 00:00:20,279
what you do for prop and back prop and I

9
00:00:18,600 --> 00:00:22,680
think because it's been difficult to

10
00:00:20,279 --> 00:00:24,600
convey intuitions about what these

11
00:00:22,680 --> 00:00:27,150
equations are doing really great and

12
00:00:24,600 --> 00:00:29,730
descends on a very complex function the

13
00:00:27,150 --> 00:00:32,399
analogy that is like the brain has

14
00:00:29,730 --> 00:00:34,440
become really an oversimplified

15
00:00:32,399 --> 00:00:36,660
explanation for what this is doing but

16
00:00:34,440 --> 00:00:39,149
the simplicity of this makes it you know

17
00:00:36,660 --> 00:00:41,399
kind of seductive for people to just say

18
00:00:39,149 --> 00:00:43,290
it publicly as well as the media to

19
00:00:41,399 --> 00:00:45,899
report it and certainly called the

20
00:00:43,290 --> 00:00:48,469
popular imagination and there is a very

21
00:00:45,899 --> 00:00:51,780
loose analogy between let's say a

22
00:00:48,469 --> 00:00:55,949
logistic regression unit with a sigmoid

23
00:00:51,780 --> 00:00:58,620
activation function and here's a cartoon

24
00:00:55,949 --> 00:01:01,890
of a single neuron in the brain in this

25
00:00:58,620 --> 00:01:03,600
picture of a biological neuron on this

26
00:01:01,890 --> 00:01:06,330
neuron which is a cell in your brain

27
00:01:03,600 --> 00:01:09,420
receives electric signals from you know

28
00:01:06,330 --> 00:01:12,299
other neurons mu X 1 X 2 X 3 or maybe

29
00:01:09,420 --> 00:01:15,119
from other neurons a 1 a 2 a 3 there's a

30
00:01:12,299 --> 00:01:17,850
simple thresholded computation and then

31
00:01:15,119 --> 00:01:20,700
if this neuron fires it sends a pulse of

32
00:01:17,850 --> 00:01:23,640
electricity down the axon down this long

33
00:01:20,700 --> 00:01:27,450
wire perhaps to other neurons so there

34
00:01:23,640 --> 00:01:29,700
is a very simplistic analogy between a

35
00:01:27,450 --> 00:01:32,189
single logistic unit between a single

36
00:01:29,700 --> 00:01:35,100
neuron and network and a biological

37
00:01:32,189 --> 00:01:37,500
neuron like that shown on a right but I

38
00:01:35,100 --> 00:01:40,170
think that today even neuroscientists

39
00:01:37,500 --> 00:01:42,720
have almost no idea what even a single

40
00:01:40,170 --> 00:01:44,850
neuron is doing a single neuron appears

41
00:01:42,720 --> 00:01:47,700
to be much more complex than we are able

42
00:01:44,850 --> 00:01:50,700
to characterize with neuroscience and

43
00:01:47,700 --> 00:01:53,220
while some of what is doing is a little

44
00:01:50,700 --> 00:01:55,079
bit like logistic regression there's

45
00:01:53,220 --> 00:01:57,390
still a lot about what even a single

46
00:01:55,079 --> 00:02:00,119
neuron does that no one there no human

47
00:01:57,390 --> 00:02:01,979
today understands for example exactly

48
00:02:00,119 --> 00:02:03,960
how neurons in the human brain learn

49
00:02:01,979 --> 00:02:07,170
this is still a very mysterious process

50
00:02:03,960 --> 00:02:08,970
and it's completely unclear today

51
00:02:07,170 --> 00:02:10,440
whether the human brain uses an

52
00:02:08,970 --> 00:02:12,120
algorithm does anything like back

53
00:02:10,440 --> 00:02:13,769
propagation or gradient descent

54
00:02:12,120 --> 00:02:16,200
or if there's some fundamentally

55
00:02:13,769 --> 00:02:19,950
different learning principle that the

56
00:02:16,200 --> 00:02:22,349
human brain uses so when I think of deep

57
00:02:19,950 --> 00:02:24,420
learning I think of it as being very

58
00:02:22,349 --> 00:02:27,000
good and learning very flexible

59
00:02:24,420 --> 00:02:29,280
functions very complex functions to

60
00:02:27,000 --> 00:02:31,290
learn X to Y mappings to learn

61
00:02:29,280 --> 00:02:34,319
input-output mappings in supervised

62
00:02:31,290 --> 00:02:36,750
learning and whereas D is like the brain

63
00:02:34,319 --> 00:02:39,659
analogy maybe that was useful once I

64
00:02:36,750 --> 00:02:41,970
think the field has moved to the point

65
00:02:39,659 --> 00:02:44,069
where that analogy is breaking down and

66
00:02:41,970 --> 00:02:47,280
I tend not to use that analogy much

67
00:02:44,069 --> 00:02:49,799
anymore so that's it so neural networks

68
00:02:47,280 --> 00:02:51,810
and their brain I do think that maybe

69
00:02:49,799 --> 00:02:53,849
the field of computer vision has taken a

70
00:02:51,810 --> 00:02:56,069
bit more inspiration from the human

71
00:02:53,849 --> 00:02:58,950
brains and other disciplines that also

72
00:02:56,069 --> 00:03:00,840
apply to learning but I personally use

73
00:02:58,950 --> 00:03:04,019
the analogy you know to the human brain

74
00:03:00,840 --> 00:03:04,890
less than I used to so that's it for

75
00:03:04,019 --> 00:03:07,409
this video

76
00:03:04,890 --> 00:03:09,090
you now know how to implement for prop

77
00:03:07,409 --> 00:03:11,609
and back prop in gradient descent even

78
00:03:09,090 --> 00:03:13,620
for deep neural networks best of luck

79
00:03:11,609 --> 00:03:15,629
with the pro exercise and I look forward

80
00:03:13,620 --> 00:03:18,530
to sharing more of these ideas of you in

81
00:03:15,629 --> 00:03:18,530
the second course